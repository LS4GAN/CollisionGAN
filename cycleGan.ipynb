{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cycle GAN\n",
    "A simpler CycleGAN constructed following [this repo](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix).\n",
    "\n",
    "Modify it to handle the single-channeled TPC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from torch.nn import init\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "# from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sys.path.append('/sdcc/u/yhuang2/PROJs/GAN/collisionGAN/utils')\n",
    "import network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/sdcc/u/yhuang2/PROJs/GAN/collisionGAN//models')\n",
    "from resnet import ResnetGenerator\n",
    "from alexnet import AlexNetDiscriminator\n",
    "from base_model import BaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple option class\n",
    "(Let us get fancier later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class options:\n",
    "    \"\"\"\n",
    "    I just list all the options here \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # data\n",
    "        self.dataroot = '/sdcc/u/yhuang2/PROJs/GAN/datasets/ls4gan/toyzero_cropped/toyzero_2021-06-29_safi_U/'\n",
    "        self.phase = 'train'\n",
    "        assert self.phase in ['train', 'test'], \"Invalid phase, choose from ['train', 'test']\"\n",
    "        self.max_dataset_size = 1000\n",
    "        \n",
    "        # base_model\n",
    "        self.cuda=True\n",
    "        self.isTrain = True if self.phase == 'train' else False\n",
    "        self.checkpoint_dir = '/sdcc/u/yhuang2/PROJs/GAN/collisionGAN/checkpoints'\n",
    "        self.experiment_name = 'experiment'\n",
    "        \n",
    "        # Training\n",
    "        self.netG = 'resnet_6blocks'\n",
    "        self.batch_size = 32\n",
    "        self.num_workers = 1\n",
    "        self.input_nc = 1\n",
    "        self.output_nc = 1\n",
    "        self.ngf = 64\n",
    "        self.norm_type = 'instance'\n",
    "        self.use_dropout = False\n",
    "        self.gan_mode = 'vanilla'\n",
    "        self.lr = 0.0002\n",
    "        self.lr_policy = 'linear'\n",
    "        self.lr_linear = 100\n",
    "        self.lr_step = 50\n",
    "        self.epochs = 200\n",
    "        self.beta1 = .5\n",
    "        \n",
    "        self.lambda_A = 10\n",
    "        self.lambda_B = 10\n",
    "        self.lambda_identity = .5\n",
    "        self.pool_size = 50\n",
    "        \n",
    "opt = options()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct generators and discriminators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_G(\n",
    "    input_nc,\n",
    "    output_nc,\n",
    "    ngf,\n",
    "    netG,\n",
    "    norm_type='instance', \n",
    "    use_dropout=False, \n",
    "    cuda=True,\n",
    "    net_name='network',\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create a generator:\n",
    "    \n",
    "    Parameters:\n",
    "        1. input_nc (int): number of input channels\n",
    "        2. output_nc (int): number of output channels\n",
    "        3. ngf (int): base number of filters in the conv layers\n",
    "        4. netG (str), the architecture's name: resnet_9blocks | resnet_6blocks \n",
    "            (I will implement remaining later),\n",
    "        5. norm_type (str): the name of the normalization: instance | batch | none\n",
    "        6. use_dropout (bool): whether to use dropout.\n",
    "    \n",
    "    Returns: A generator\n",
    "    \"\"\"\n",
    "    \n",
    "    # net = None,\n",
    "    norm_layer = network.get_norm_layer(norm_type=norm_type)\n",
    "    \n",
    "    if netG == 'resent_9blocks':\n",
    "        net = ResnetGenerator(\n",
    "            input_nc, \n",
    "            output_nc, \n",
    "            ngf, \n",
    "            norm_layer=norm_layer, \n",
    "            use_dropout=use_dropout,\n",
    "            n_blocks=9\n",
    "        )\n",
    "    elif netG == 'resnet_6blocks':\n",
    "        net = ResnetGenerator(\n",
    "            input_nc, \n",
    "            output_nc, \n",
    "            ngf, \n",
    "            norm_layer=norm_layer, \n",
    "            use_dropout=use_dropout,\n",
    "            n_blocks=6\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(f'Generator model name {netG} is not implemented')\n",
    "    \n",
    "    if cuda:\n",
    "        assert torch.cuda.is_available(), \"Cuda is not available\"\n",
    "        net.to('cuda')\n",
    "    network.init_weights(net, net_name=net_name)\n",
    "    \n",
    "    return net\n",
    "\n",
    "\n",
    "def define_D(input_nc, cuda=True, net_name='network'):\n",
    "    \"\"\"\n",
    "    Create an very simple AlexNet discriminator\n",
    "\n",
    "    Parameters:\n",
    "        1. input_nc (int): number of channels in input images\n",
    "        \n",
    "    Returns: A discriminator\n",
    "    \"\"\"\n",
    "    net = AlexNetDiscriminator(input_channels=input_nc)\n",
    "    if cuda:\n",
    "        assert torch.cuda.is_available(), \"Cuda is not available\"\n",
    "        net.to('cuda')\n",
    "    network.init_weights(net, net_name=net_name)\n",
    "    return net\n",
    "\n",
    "\n",
    "# def define_D(\n",
    "#     input_nc, \n",
    "#     ndf,\n",
    "#     netD,\n",
    "#     n_layer=3,\n",
    "#     norm_type='instance',\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Create a discriminator\n",
    "\n",
    "#     Parameters:\n",
    "#         1. input_nc (int): number of channels in input images\n",
    "#         2. ndf (int): number of filters in the first conv layer\n",
    "#         3. netD (str): the architecture's name: basic | n_layers | pixel\n",
    "#         4.n_layers_D (int): the number of conv layers in the discriminator; \n",
    "#             effective when netD=='n_layers'\n",
    "#         5. norm_type (str): the type of normalization layers used in the network.\n",
    "\n",
    "#     Returns: A discriminator\n",
    "#     \"\"\"\n",
    "    \n",
    "#     norm_layer = network.get_norm_layer(norm_type=norm_type)\n",
    "#     net = NLayerDiscriminator(input_nc, ndf, n_layers=n_layers, norm_layer=norm_layer)\n",
    "#     return network.init_weight(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Define different GAN objectives.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gan_mode):\n",
    "        \"\"\" \n",
    "        Initialize the GANLoss class.\n",
    "\n",
    "        Parameters:\n",
    "            gan_mode (str) -- the type of GAN objective. \n",
    "                It currently supports \n",
    "                    - vanilla; \n",
    "                    - lsgan; \n",
    "                    - wgangp.\n",
    "\n",
    "        NOTE: DO NOT use sigmoid as the last layer of Discriminator.\n",
    "            - vanilla handles it with BCEWithLogitsLoss.\n",
    "            - lsgan needs no sigmoid. \n",
    "        \"\"\"\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.register_buffer('real_label', torch.tensor(1.0))\n",
    "        self.register_buffer('fake_label', torch.tensor(0.0))\n",
    "        self.gan_mode = gan_mode\n",
    "        if gan_mode == 'lsgan':\n",
    "            self.loss = nn.MSELoss()\n",
    "        elif gan_mode == 'vanilla':\n",
    "            self.loss = nn.BCEWithLogitsLoss()\n",
    "        elif gan_mode == 'wgangp':\n",
    "            self.loss = None\n",
    "        else:\n",
    "            raise NotImplementedError('gan mode %s not implemented' % gan_mode)\n",
    "\n",
    "    def __call__(self, prediction, target_is_real):\n",
    "        \"\"\"\n",
    "        Calculate loss given Discriminator's output and grount truth labels.\n",
    "\n",
    "        Parameters:\n",
    "            prediction (tensor) -- prediction from a discriminator\n",
    "            target_is_real (bool) -- if the ground truth label is for real images or fake images\n",
    "\n",
    "        Returns:\n",
    "            the calculated loss.\n",
    "        \"\"\"\n",
    "        if self.gan_mode in ['lsgan', 'vanilla']:\n",
    "            target_tensor = self.real_label if target_is_real else self.fake_label\n",
    "            loss = self.loss(prediction, target_tensor.expand_as(prediction))\n",
    "        elif self.gan_mode == 'wgangp':\n",
    "            L = prediction.mean()\n",
    "            loss = -L if target_is_real else L\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGAN(BaseModel):\n",
    "    def __init__(self, opt):\n",
    "        super(CycleGAN, self).__init__(opt)\n",
    "        self.device = 'cuda' if opt.cuda else 'cpu'\n",
    "        \n",
    "        self.loss_names = ['D_A', 'D_B', 'G_A', 'G_B', 'cycle_A', 'cycle_B', 'idt_A', 'idt_B']\n",
    "        if self.isTrain:\n",
    "            self.model_names = ['G_A', 'G_B', 'D_A', 'D_B']\n",
    "        else:\n",
    "            self.model_names = ['G_A', 'G_B']\n",
    "            \n",
    "        # generator networks\n",
    "        self.netG_A = define_G(\n",
    "            opt.input_nc,\n",
    "            opt.output_nc,\n",
    "            opt.ngf,\n",
    "            opt.netG,\n",
    "            opt.norm_type,\n",
    "            opt.use_dropout,\n",
    "            cuda=opt.cuda,\n",
    "            net_name='generator A->B',\n",
    "        )\n",
    "        self.netG_B = define_G(\n",
    "            opt.output_nc,\n",
    "            opt.input_nc, \n",
    "            opt.ngf,\n",
    "            opt.netG,\n",
    "            opt.norm_type,\n",
    "            opt.use_dropout,\n",
    "            cuda=opt.cuda,\n",
    "            net_name='generator B->A',\n",
    "        )\n",
    "        \n",
    "        # discriminator networks, criterions, and optimizers for training\n",
    "        if opt.isTrain:\n",
    "#             self.netD_A = define_D(\n",
    "#                 opt.output_nc,\n",
    "#                 opt.ndf, # number of discriminator filters in the first conv layer\n",
    "#                 opt.netD,\n",
    "#                 opt.n_layers_D,\n",
    "#                 opt.norm_type,\n",
    "#             )\n",
    "#             self.netD_B = define_D(\n",
    "#                 opt.input_nc,\n",
    "#                 opt.ndf, # number of discriminator filters in the first conv layer\n",
    "#                 opt.netD,\n",
    "#                 opt.n_layers_D,\n",
    "#                 opt.norm_type,\n",
    "#             )\n",
    "            self.netD_A = define_D(opt.output_nc, cuda=opt.cuda, net_name='discriminator A')\n",
    "            self.netD_B = define_D(opt.input_nc, cuda=opt.cuda, net_name='discriminator B')\n",
    "            \n",
    "            if opt.lambda_identity > 0:\n",
    "                assert(opt.input_nc == opt.output_nc)\n",
    "            \n",
    "            # fake images buffers\n",
    "            self.fake_A_pool = network.ImagePool(opt.pool_size)\n",
    "            self.fake_B_pool = network.ImagePool(opt.pool_size)\n",
    "            \n",
    "            # Loss functions\n",
    "            self.criterionGAN = GANLoss(opt.gan_mode).to(self.device)\n",
    "            self.criterionCycle = torch.nn.L1Loss()\n",
    "            self.criterionIdt = torch.nn.L1Loss()\n",
    "            \n",
    "            # Optimizers\n",
    "            self.optimizer_G = torch.optim.Adam(\n",
    "                itertools.chain(self.netG_A.parameters(), self.netG_B.parameters()), \n",
    "                lr=opt.lr, \n",
    "                betas=(opt.beta1, .999)\n",
    "            )\n",
    "            self.optimizer_D = torch.optim.Adam(\n",
    "                itertools.chain(self.netD_A.parameters(), self.netD_B.parameters()), \n",
    "                lr=opt.lr, \n",
    "                betas=(opt.beta1, .999)\n",
    "            )\n",
    "            self.optimizers = [self.optimizer_G, self.optimizer_D]\n",
    "            if self.isTrain:\n",
    "                self.schedulers = [network.get_scheduler(optimizer, opt) for optimizer in self.optimizers]\n",
    "\n",
    "    def set_input(self, data):\n",
    "        self.real_A = data['A'].to(self.device)\n",
    "        self.real_B = data['B'].to(self.device)\n",
    "   \n",
    "    def forward(self):\n",
    "        self.fake_B = self.netG_A(self.real_A) # G_A(A)\n",
    "        self.rec_A = self.netG_B(self.fake_B)  # G_B(G_A(A))\n",
    "        self.fake_A = self.netG_B(self.real_B) # G_B(B)\n",
    "        self.rec_B = self.netG_A(self.fake_A)  # G_A(G_B(B))\n",
    "        \n",
    "    def backward_D_basic(self, netD, real, fake):\n",
    "        \"\"\"\n",
    "        Calculate GAN loss for the discriminator\n",
    "        \n",
    "        Parameters:\n",
    "            1. netD (network): the discriminator D\n",
    "            2. real (tensor): real images\n",
    "            3. fake (tensor): images generated by a generator\n",
    "        \n",
    "        Retrun:\n",
    "            the discriminator loss.\n",
    "        \"\"\"\n",
    "        # Real \n",
    "        loss_D_real = self.criterionGAN(netD(real), True)\n",
    "        \n",
    "        # Fake\n",
    "        loss_D_fake = self.criterionGAN(netD(fake.detach()), False)\n",
    "        \n",
    "        # Combine loss and calculate gradients\n",
    "        loss_D = (loss_D_real + loss_D_fake) * .5\n",
    "        loss_D.backward()\n",
    "        \n",
    "        return loss_D\n",
    "\n",
    "    def backward_D_A(self):\n",
    "        \"\"\"\n",
    "        Calculate GAN loss for the discriminator D_A\n",
    "        \"\"\"\n",
    "        self.loss_D_A = self.backward_D_basic(\n",
    "            self.netD_A,\n",
    "            self.real_B,\n",
    "            # the collection of fake images is a \n",
    "            # mixture of old and new fake images\n",
    "            self.fake_B_pool.query(self.fake_B)\n",
    "        )\n",
    "\n",
    "    def backward_D_B(self):\n",
    "        \"\"\"\n",
    "        Calculate GAN loss for the discriminator D_B\n",
    "        \"\"\"\n",
    "        self.loss_D_B = self.backward_D_basic(\n",
    "            self.netD_B,\n",
    "            self.real_A,\n",
    "            # the collection of fake images is a \n",
    "            # mixture of old and new fake images\n",
    "            self.fake_A_pool.query(self.fake_A)\n",
    "        )\n",
    "\n",
    "    def backward_G(self):\n",
    "        \"\"\"\n",
    "        Calculate the loss for generators G_A and G_B\n",
    "        \"\"\"\n",
    "        lambda_idt = self.opt.lambda_identity # it is a multplier to lambda_A and lambda_B\n",
    "        lambda_A, lambda_B = self.opt.lambda_A, self.opt.lambda_B\n",
    "        \n",
    "        # Identity loss\n",
    "        self.loss_idt_A, self.loss_idt_B = 0, 0\n",
    "        if lambda_idt > 0:\n",
    "            self.idt_A = self.netG_A(self.real_B)\n",
    "            self.loss_idt_A = self.criterionIdt(self.idt_A, self.real_B) * lambda_B * lambda_idt\n",
    "            self.idt_B = self.netG_B(self.real_A)\n",
    "            self.loss_idt_B = self.criterionIdt(self.idt_B, self.real_A) * lambda_A * lambda_idt\n",
    "        \n",
    "        # GAN losses\n",
    "        self.loss_G_A = self.criterionGAN(self.netD_A(self.fake_B), True)\n",
    "        self.loss_G_B = self.criterionGAN(self.netD_B(self.fake_A), True)\n",
    "        \n",
    "        # Cycle losses:\n",
    "        self.loss_cycle_A = self.criterionCycle(self.rec_A, self.real_A) * lambda_A\n",
    "        self.loss_cycle_B = self.criterionCycle(self.rec_B, self.real_B) * lambda_B\n",
    "        \n",
    "        # Combine and backpropagate\n",
    "        self.loss_G = self.loss_G_A + self.loss_G_B \\\n",
    "                    + self.loss_cycle_A + self.loss_cycle_B \\\n",
    "                    + self.loss_idt_A + self.loss_idt_B\n",
    "        self.loss_G.backward()\n",
    " \n",
    "    def optimize_parameters(self):\n",
    "        \"\"\"\n",
    "        Calculate losses and gradients, and update network weights.\n",
    "        \"\"\"\n",
    "        self.forward()\n",
    "        \n",
    "        # Optimize generators\n",
    "        for net in [self.netD_A, self.netD_B]:\n",
    "            for param in net.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.optimizer_G.zero_grad()\n",
    "        self.backward_G()\n",
    "        self.optimizer_G.step()\n",
    "        \n",
    "        # Optimize discriminators\n",
    "        for net in [self.netD_A, self.netD_B]:\n",
    "            for param in net.parameters():\n",
    "                param.requires_grad = True\n",
    "                \n",
    "        self.optimizer_D.zero_grad()\n",
    "        self.backward_D_A()\n",
    "        self.backward_D_B()\n",
    "        self.optimizer_D.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_fnames(dirname, max_dataset_size=float('inf')):\n",
    "    \"\"\"\n",
    "    load image fnames.\n",
    "    If max_dataset_size is not infinity and is less than all available images,\n",
    "    return a random subset of max_dataset_size image fnames.\n",
    "    \"\"\"\n",
    "    assert Path(dirname).exists(), f\"{dirname} doesn't exist\"\n",
    "    image_fnames = np.array(list(Path(dirname).glob('*npz')))\n",
    "    \n",
    "    if max_dataset_size != float('inf') and max_dataset_size < len(image_fnames):\n",
    "        indices = np.arange(len(image_fnames))\n",
    "        np.random.shuffle(indices)\n",
    "        indices = indices[:max_dataset_size]\n",
    "        image_fnames = image_fnames[indices]\n",
    "    return image_fnames\n",
    "\n",
    "\n",
    "class toyzero_dataset(Dataset):\n",
    "    def __init__(self, opt):\n",
    "        super(toyzero_dataset, self).__init__()\n",
    "        dir_A = Path(opt.dataroot)/f'{opt.phase}A'\n",
    "        dir_B = Path(opt.dataroot)/f'{opt.phase}B'\n",
    "        self.image_fnames_A = load_image_fnames(dir_A, opt.max_dataset_size)\n",
    "        self.image_fnames_B = load_image_fnames(dir_B, opt.max_dataset_size)\n",
    "        self.size_A = len(self.image_fnames_A)\n",
    "        self.size_B = len(self.image_fnames_B)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(self.size_A, self.size_B)\n",
    "    \n",
    "    def _load(self, image_fname):\n",
    "        image = np.load(image_fname)\n",
    "        image = image[image.files[0]]\n",
    "        image = np.expand_dims(np.float32(image), 0)\n",
    "        return image\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        index_A = index % self.size_A\n",
    "        index_B = np.random.randint(0, self.size_B - 1) # inclusive end\n",
    "        image_A = self._load(self.image_fnames_A[index_A])\n",
    "        image_B = self._load(self.image_fnames_B[index_B])\n",
    "        \n",
    "        return {'A': image_A, 'B': image_B}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches = 32\n"
     ]
    }
   ],
   "source": [
    "dataset = toyzero_dataset(opt)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=opt.batch_size,\n",
    "    num_workers=opt.num_workers,\n",
    "    shuffle=True\n",
    ")\n",
    "print(f'number of batches = {len(dataloader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize generator A->B with normal distribution\n",
      "initialize generator B->A with normal distribution\n",
      "initialize discriminator A with normal distribution\n",
      "initialize discriminator B with normal distribution\n"
     ]
    }
   ],
   "source": [
    "model = CycleGAN(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 / 200\n",
      "\tD_A, 0.12154\n",
      "\tD_B, 0.09191\n",
      "\tG_A, 1.50898\n",
      "\tG_B, 1.68529\n",
      "\tcycle_A, 2.86329\n",
      "\tcycle_B, 2.99282\n",
      "\tidt_A, 1.49036\n",
      "\tidt_B, 1.41479\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "2 / 200\n",
      "\tD_A, 0.08734\n",
      "\tD_B, 0.06404\n",
      "\tG_A, 1.66890\n",
      "\tG_B, 3.45798\n",
      "\tcycle_A, 1.78605\n",
      "\tcycle_B, 2.98108\n",
      "\tidt_A, 1.48043\n",
      "\tidt_B, 0.88862\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "3 / 200\n",
      "\tD_A, 0.35954\n",
      "\tD_B, 0.24107\n",
      "\tG_A, 1.09639\n",
      "\tG_B, 2.27057\n",
      "\tcycle_A, 2.67571\n",
      "\tcycle_B, 3.00795\n",
      "\tidt_A, 1.51717\n",
      "\tidt_B, 1.32266\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "4 / 200\n",
      "\tD_A, 0.65901\n",
      "\tD_B, 0.63771\n",
      "\tG_A, 0.96532\n",
      "\tG_B, 3.08482\n",
      "\tcycle_A, 1.74189\n",
      "\tcycle_B, 2.92602\n",
      "\tidt_A, 1.46199\n",
      "\tidt_B, 0.86122\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "5 / 200\n",
      "\tD_A, 0.14052\n",
      "\tD_B, 0.22439\n",
      "\tG_A, 1.71800\n",
      "\tG_B, 4.33523\n",
      "\tcycle_A, 2.54174\n",
      "\tcycle_B, 3.06497\n",
      "\tidt_A, 1.55416\n",
      "\tidt_B, 1.23928\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "6 / 200\n",
      "\tD_A, 0.33007\n",
      "\tD_B, 0.20137\n",
      "\tG_A, 1.57826\n",
      "\tG_B, 4.25931\n",
      "\tcycle_A, 2.32158\n",
      "\tcycle_B, 3.20161\n",
      "\tidt_A, 1.57874\n",
      "\tidt_B, 1.16688\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "7 / 200\n",
      "\tD_A, 0.51832\n",
      "\tD_B, 3.70965\n",
      "\tG_A, 1.02779\n",
      "\tG_B, 10.22215\n",
      "\tcycle_A, 3.23605\n",
      "\tcycle_B, 3.17274\n",
      "\tidt_A, 1.58174\n",
      "\tidt_B, 1.64003\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "8 / 200\n",
      "\tD_A, 0.53188\n",
      "\tD_B, 0.03497\n",
      "\tG_A, 0.53851\n",
      "\tG_B, 2.36004\n",
      "\tcycle_A, 2.94994\n",
      "\tcycle_B, 3.12880\n",
      "\tidt_A, 1.58784\n",
      "\tidt_B, 1.51871\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "9 / 200\n",
      "\tD_A, 0.64452\n",
      "\tD_B, 0.20824\n",
      "\tG_A, 0.44857\n",
      "\tG_B, 2.45258\n",
      "\tcycle_A, 2.85311\n",
      "\tcycle_B, 3.10515\n",
      "\tidt_A, 1.60934\n",
      "\tidt_B, 1.40883\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "10 / 200\n",
      "\tD_A, 0.66780\n",
      "\tD_B, 0.23863\n",
      "\tG_A, 0.57542\n",
      "\tG_B, 4.42141\n",
      "\tcycle_A, 2.24136\n",
      "\tcycle_B, 3.21039\n",
      "\tidt_A, 1.58554\n",
      "\tidt_B, 1.10285\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "11 / 200\n",
      "\tD_A, 0.10970\n",
      "\tD_B, 0.24491\n",
      "\tG_A, 4.48903\n",
      "\tG_B, 1.94432\n",
      "\tcycle_A, 2.78564\n",
      "\tcycle_B, 3.28682\n",
      "\tidt_A, 1.61451\n",
      "\tidt_B, 1.34550\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "12 / 200\n",
      "\tD_A, 0.17355\n",
      "\tD_B, 0.67044\n",
      "\tG_A, 8.38668\n",
      "\tG_B, 0.63191\n",
      "\tcycle_A, 1.70107\n",
      "\tcycle_B, 3.23710\n",
      "\tidt_A, 1.60933\n",
      "\tidt_B, 0.83003\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "13 / 200\n",
      "\tD_A, 0.46580\n",
      "\tD_B, 0.53609\n",
      "\tG_A, 0.67162\n",
      "\tG_B, 0.75048\n",
      "\tcycle_A, 2.82522\n",
      "\tcycle_B, 3.24401\n",
      "\tidt_A, 1.61969\n",
      "\tidt_B, 1.38306\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "14 / 200\n",
      "\tD_A, 0.58587\n",
      "\tD_B, 0.55213\n",
      "\tG_A, 0.80716\n",
      "\tG_B, 0.93622\n",
      "\tcycle_A, 2.01914\n",
      "\tcycle_B, 3.05658\n",
      "\tidt_A, 1.54373\n",
      "\tidt_B, 1.03507\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "15 / 200\n",
      "\tD_A, 0.52803\n",
      "\tD_B, 0.51497\n",
      "\tG_A, 1.35222\n",
      "\tG_B, 0.98279\n",
      "\tcycle_A, 2.33712\n",
      "\tcycle_B, 2.99138\n",
      "\tidt_A, 1.57116\n",
      "\tidt_B, 1.24491\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "16 / 200\n",
      "\tD_A, 0.72041\n",
      "\tD_B, 0.46592\n",
      "\tG_A, 0.58353\n",
      "\tG_B, 1.27646\n",
      "\tcycle_A, 1.49780\n",
      "\tcycle_B, 3.01942\n",
      "\tidt_A, 1.58657\n",
      "\tidt_B, 0.77875\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "17 / 200\n",
      "\tD_A, 0.51235\n",
      "\tD_B, 0.23727\n",
      "\tG_A, 1.31528\n",
      "\tG_B, 2.53462\n",
      "\tcycle_A, 3.14716\n",
      "\tcycle_B, 3.13891\n",
      "\tidt_A, 1.55984\n",
      "\tidt_B, 1.60015\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "18 / 200\n",
      "\tD_A, 0.51314\n",
      "\tD_B, 0.42145\n",
      "\tG_A, 1.33589\n",
      "\tG_B, 1.19895\n",
      "\tcycle_A, 1.69987\n",
      "\tcycle_B, 3.05630\n",
      "\tidt_A, 1.56623\n",
      "\tidt_B, 0.83962\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "19 / 200\n",
      "\tD_A, 0.55338\n",
      "\tD_B, 0.10502\n",
      "\tG_A, 1.33915\n",
      "\tG_B, 6.26479\n",
      "\tcycle_A, 1.24103\n",
      "\tcycle_B, 3.02603\n",
      "\tidt_A, 1.52946\n",
      "\tidt_B, 0.61621\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "20 / 200\n",
      "\tD_A, 0.61640\n",
      "\tD_B, 0.59368\n",
      "\tG_A, 1.22315\n",
      "\tG_B, 1.50518\n",
      "\tcycle_A, 3.03023\n",
      "\tcycle_B, 3.02957\n",
      "\tidt_A, 1.52669\n",
      "\tidt_B, 1.50118\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "21 / 200\n",
      "\tD_A, 0.58299\n",
      "\tD_B, 0.07466\n",
      "\tG_A, 1.05901\n",
      "\tG_B, 17.78309\n",
      "\tcycle_A, 3.32149\n",
      "\tcycle_B, 3.17343\n",
      "\tidt_A, 1.58662\n",
      "\tidt_B, 1.65640\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "22 / 200\n",
      "\tD_A, 0.35300\n",
      "\tD_B, 0.53659\n",
      "\tG_A, 2.02608\n",
      "\tG_B, 2.87810\n",
      "\tcycle_A, 1.83268\n",
      "\tcycle_B, 3.26893\n",
      "\tidt_A, 1.59613\n",
      "\tidt_B, 0.89167\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "23 / 200\n",
      "\tD_A, 0.09352\n",
      "\tD_B, 0.44614\n",
      "\tG_A, 2.70616\n",
      "\tG_B, 2.40417\n",
      "\tcycle_A, 2.67623\n",
      "\tcycle_B, 3.26672\n",
      "\tidt_A, 1.58884\n",
      "\tidt_B, 1.30706\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "24 / 200\n",
      "\tD_A, 1.81391\n",
      "\tD_B, 0.64710\n",
      "\tG_A, 0.08250\n",
      "\tG_B, 0.67395\n",
      "\tcycle_A, 1.88935\n",
      "\tcycle_B, 3.19847\n",
      "\tidt_A, 1.57455\n",
      "\tidt_B, 0.92131\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "25 / 200\n",
      "\tD_A, 0.48155\n",
      "\tD_B, 0.59848\n",
      "\tG_A, 3.01500\n",
      "\tG_B, 0.72974\n",
      "\tcycle_A, 2.68000\n",
      "\tcycle_B, 3.17855\n",
      "\tidt_A, 1.57527\n",
      "\tidt_B, 1.33891\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "26 / 200\n",
      "\tD_A, 0.69080\n",
      "\tD_B, 0.68373\n",
      "\tG_A, 0.65742\n",
      "\tG_B, 0.62692\n",
      "\tcycle_A, 2.23584\n",
      "\tcycle_B, 3.22732\n",
      "\tidt_A, 1.59460\n",
      "\tidt_B, 1.10767\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "27 / 200\n",
      "\tD_A, 0.68955\n",
      "\tD_B, 0.67886\n",
      "\tG_A, 0.66655\n",
      "\tG_B, 0.65738\n",
      "\tcycle_A, 1.84113\n",
      "\tcycle_B, 3.02984\n",
      "\tidt_A, 1.50723\n",
      "\tidt_B, 0.91589\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "28 / 200\n",
      "\tD_A, 0.68414\n",
      "\tD_B, 0.68685\n",
      "\tG_A, 0.63159\n",
      "\tG_B, 0.65048\n",
      "\tcycle_A, 2.51472\n",
      "\tcycle_B, 2.89002\n",
      "\tidt_A, 1.43705\n",
      "\tidt_B, 1.27266\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "29 / 200\n",
      "\tD_A, 0.71721\n",
      "\tD_B, 0.68709\n",
      "\tG_A, 0.60017\n",
      "\tG_B, 0.65157\n",
      "\tcycle_A, 2.10069\n",
      "\tcycle_B, 2.87211\n",
      "\tidt_A, 1.43230\n",
      "\tidt_B, 1.06084\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "30 / 200\n",
      "\tD_A, 0.61347\n",
      "\tD_B, 0.66322\n",
      "\tG_A, 0.56461\n",
      "\tG_B, 0.64981\n",
      "\tcycle_A, 2.46170\n",
      "\tcycle_B, 2.99053\n",
      "\tidt_A, 1.49768\n",
      "\tidt_B, 1.23053\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "31 / 200\n",
      "\tD_A, 0.72959\n",
      "\tD_B, 0.57895\n",
      "\tG_A, 0.40797\n",
      "\tG_B, 0.73144\n",
      "\tcycle_A, 1.82580\n",
      "\tcycle_B, 3.13440\n",
      "\tidt_A, 1.56940\n",
      "\tidt_B, 0.86932\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "32 / 200\n",
      "\tD_A, 0.68297\n",
      "\tD_B, 0.45701\n",
      "\tG_A, 0.67265\n",
      "\tG_B, 1.27728\n",
      "\tcycle_A, 2.25650\n",
      "\tcycle_B, 2.90649\n",
      "\tidt_A, 1.44384\n",
      "\tidt_B, 1.15684\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "33 / 200\n",
      "\tD_A, 0.67819\n",
      "\tD_B, 0.33810\n",
      "\tG_A, 0.59475\n",
      "\tG_B, 1.98881\n",
      "\tcycle_A, 1.81362\n",
      "\tcycle_B, 3.13190\n",
      "\tidt_A, 1.55932\n",
      "\tidt_B, 0.91742\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "34 / 200\n",
      "\tD_A, 0.68615\n",
      "\tD_B, 0.21589\n",
      "\tG_A, 0.64330\n",
      "\tG_B, 3.18076\n",
      "\tcycle_A, 3.51194\n",
      "\tcycle_B, 2.95852\n",
      "\tidt_A, 1.46531\n",
      "\tidt_B, 1.80326\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "35 / 200\n",
      "\tD_A, 0.67924\n",
      "\tD_B, 0.01529\n",
      "\tG_A, 0.63721\n",
      "\tG_B, 8.67744\n",
      "\tcycle_A, 2.28958\n",
      "\tcycle_B, 2.89834\n",
      "\tidt_A, 1.42582\n",
      "\tidt_B, 1.15607\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "36 / 200\n",
      "\tD_A, 0.64733\n",
      "\tD_B, 0.03517\n",
      "\tG_A, 0.66004\n",
      "\tG_B, 8.01449\n",
      "\tcycle_A, 2.03615\n",
      "\tcycle_B, 2.83260\n",
      "\tidt_A, 1.40477\n",
      "\tidt_B, 1.02443\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "37 / 200\n",
      "\tD_A, 0.51386\n",
      "\tD_B, 0.35903\n",
      "\tG_A, 0.66570\n",
      "\tG_B, 3.53875\n",
      "\tcycle_A, 2.21414\n",
      "\tcycle_B, 2.83168\n",
      "\tidt_A, 1.40034\n",
      "\tidt_B, 1.11046\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "38 / 200\n",
      "\tD_A, 1.20450\n",
      "\tD_B, 0.03867\n",
      "\tG_A, 0.28825\n",
      "\tG_B, 7.84644\n",
      "\tcycle_A, 4.37163\n",
      "\tcycle_B, 3.14359\n",
      "\tidt_A, 1.53875\n",
      "\tidt_B, 2.19029\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "39 / 200\n",
      "\tD_A, 0.44036\n",
      "\tD_B, 0.76431\n",
      "\tG_A, 2.11591\n",
      "\tG_B, 0.68223\n",
      "\tcycle_A, 2.28239\n",
      "\tcycle_B, 3.16275\n",
      "\tidt_A, 1.56296\n",
      "\tidt_B, 1.13698\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "40 / 200\n",
      "\tD_A, 0.68206\n",
      "\tD_B, 0.50802\n",
      "\tG_A, 0.60890\n",
      "\tG_B, 3.95870\n",
      "\tcycle_A, 2.60988\n",
      "\tcycle_B, 3.17743\n",
      "\tidt_A, 1.54576\n",
      "\tidt_B, 1.29779\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "41 / 200\n",
      "\tD_A, 0.58535\n",
      "\tD_B, 0.07172\n",
      "\tG_A, 0.83515\n",
      "\tG_B, 5.56187\n",
      "\tcycle_A, 1.87920\n",
      "\tcycle_B, 3.18649\n",
      "\tidt_A, 1.59512\n",
      "\tidt_B, 0.94099\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "42 / 200\n",
      "\tD_A, 0.75417\n",
      "\tD_B, 0.03240\n",
      "\tG_A, 2.10888\n",
      "\tG_B, 8.38887\n",
      "\tcycle_A, 1.81891\n",
      "\tcycle_B, 3.11115\n",
      "\tidt_A, 1.57295\n",
      "\tidt_B, 0.91282\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "43 / 200\n",
      "\tD_A, 0.09735\n",
      "\tD_B, 0.48734\n",
      "\tG_A, 32.42705\n",
      "\tG_B, 0.82969\n",
      "\tcycle_A, 2.71051\n",
      "\tcycle_B, 3.38344\n",
      "\tidt_A, 1.63378\n",
      "\tidt_B, 1.33246\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "44 / 200\n",
      "\tD_A, 0.50856\n",
      "\tD_B, 0.56123\n",
      "\tG_A, 1.19184\n",
      "\tG_B, 1.34378\n",
      "\tcycle_A, 3.00118\n",
      "\tcycle_B, 3.34680\n",
      "\tidt_A, 1.65768\n",
      "\tidt_B, 1.48205\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "45 / 200\n",
      "\tD_A, 0.21889\n",
      "\tD_B, 0.69291\n",
      "\tG_A, 2.97738\n",
      "\tG_B, 0.70037\n",
      "\tcycle_A, 2.38504\n",
      "\tcycle_B, 3.12512\n",
      "\tidt_A, 1.53269\n",
      "\tidt_B, 1.19999\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "46 / 200\n",
      "\tD_A, 0.03905\n",
      "\tD_B, 0.69382\n",
      "\tG_A, 7.76697\n",
      "\tG_B, 0.67922\n",
      "\tcycle_A, 1.82555\n",
      "\tcycle_B, 3.22533\n",
      "\tidt_A, 1.62069\n",
      "\tidt_B, 0.89384\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "47 / 200\n",
      "\tD_A, 0.18257\n",
      "\tD_B, 0.68723\n",
      "\tG_A, 1.43667\n",
      "\tG_B, 0.68890\n",
      "\tcycle_A, 4.13101\n",
      "\tcycle_B, 3.10118\n",
      "\tidt_A, 1.56462\n",
      "\tidt_B, 2.05070\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "48 / 200\n",
      "\tD_A, 0.09167\n",
      "\tD_B, 0.47540\n",
      "\tG_A, 7.42716\n",
      "\tG_B, 1.15003\n",
      "\tcycle_A, 2.74662\n",
      "\tcycle_B, 3.23440\n",
      "\tidt_A, 1.62176\n",
      "\tidt_B, 1.35292\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "49 / 200\n",
      "\tD_A, 0.20647\n",
      "\tD_B, 0.67127\n",
      "\tG_A, 3.52062\n",
      "\tG_B, 0.60236\n",
      "\tcycle_A, 3.06467\n",
      "\tcycle_B, 3.16975\n",
      "\tidt_A, 1.56542\n",
      "\tidt_B, 1.50641\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "50 / 200\n",
      "\tD_A, 0.50985\n",
      "\tD_B, 0.94449\n",
      "\tG_A, 1.13422\n",
      "\tG_B, 0.64338\n",
      "\tcycle_A, 2.87779\n",
      "\tcycle_B, 3.14267\n",
      "\tidt_A, 1.56882\n",
      "\tidt_B, 1.43009\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "51 / 200\n",
      "\tD_A, 0.12183\n",
      "\tD_B, 0.63919\n",
      "\tG_A, 8.98466\n",
      "\tG_B, 0.64824\n",
      "\tcycle_A, 1.39463\n",
      "\tcycle_B, 3.11702\n",
      "\tidt_A, 1.55041\n",
      "\tidt_B, 0.65505\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "52 / 200\n",
      "\tD_A, 0.68446\n",
      "\tD_B, 0.71845\n",
      "\tG_A, 0.63687\n",
      "\tG_B, 0.76587\n",
      "\tcycle_A, 2.65095\n",
      "\tcycle_B, 3.35503\n",
      "\tidt_A, 1.67571\n",
      "\tidt_B, 1.26338\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "53 / 200\n",
      "\tD_A, 0.68379\n",
      "\tD_B, 1.31559\n",
      "\tG_A, 0.66520\n",
      "\tG_B, 1.53091\n",
      "\tcycle_A, 1.92002\n",
      "\tcycle_B, 3.24951\n",
      "\tidt_A, 1.61442\n",
      "\tidt_B, 0.94683\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "54 / 200\n",
      "\tD_A, 0.69193\n",
      "\tD_B, 0.58357\n",
      "\tG_A, 0.68256\n",
      "\tG_B, 0.95535\n",
      "\tcycle_A, 2.44199\n",
      "\tcycle_B, 3.15347\n",
      "\tidt_A, 1.56363\n",
      "\tidt_B, 1.21535\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "55 / 200\n",
      "\tD_A, 0.65533\n",
      "\tD_B, 0.55522\n",
      "\tG_A, 0.69131\n",
      "\tG_B, 0.90966\n",
      "\tcycle_A, 1.76282\n",
      "\tcycle_B, 3.06572\n",
      "\tidt_A, 1.51513\n",
      "\tidt_B, 0.87730\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "56 / 200\n",
      "\tD_A, 0.58640\n",
      "\tD_B, 0.64310\n",
      "\tG_A, 0.76364\n",
      "\tG_B, 0.75543\n",
      "\tcycle_A, 3.21281\n",
      "\tcycle_B, 2.97908\n",
      "\tidt_A, 1.47495\n",
      "\tidt_B, 1.60570\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "57 / 200\n",
      "\tD_A, 0.44426\n",
      "\tD_B, 0.54688\n",
      "\tG_A, 0.88906\n",
      "\tG_B, 0.66806\n",
      "\tcycle_A, 2.07358\n",
      "\tcycle_B, 3.03154\n",
      "\tidt_A, 1.51319\n",
      "\tidt_B, 1.03512\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "58 / 200\n",
      "\tD_A, 0.74524\n",
      "\tD_B, 0.50377\n",
      "\tG_A, 0.32963\n",
      "\tG_B, 1.07906\n",
      "\tcycle_A, 1.61961\n",
      "\tcycle_B, 3.22708\n",
      "\tidt_A, 1.60266\n",
      "\tidt_B, 0.79941\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "59 / 200\n",
      "\tD_A, 0.69344\n",
      "\tD_B, 0.63472\n",
      "\tG_A, 0.42619\n",
      "\tG_B, 0.65930\n",
      "\tcycle_A, 1.45503\n",
      "\tcycle_B, 3.20029\n",
      "\tidt_A, 1.59332\n",
      "\tidt_B, 0.71272\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "60 / 200\n",
      "\tD_A, 0.59676\n",
      "\tD_B, 0.63887\n",
      "\tG_A, 0.65352\n",
      "\tG_B, 0.74931\n",
      "\tcycle_A, 1.79039\n",
      "\tcycle_B, 3.10621\n",
      "\tidt_A, 1.56789\n",
      "\tidt_B, 0.89762\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "61 / 200\n",
      "\tD_A, 0.58419\n",
      "\tD_B, 0.20700\n",
      "\tG_A, 0.60458\n",
      "\tG_B, 8.30037\n",
      "\tcycle_A, 2.77343\n",
      "\tcycle_B, 3.25477\n",
      "\tidt_A, 1.57296\n",
      "\tidt_B, 1.38911\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "62 / 200\n",
      "\tD_A, 0.33981\n",
      "\tD_B, 0.60508\n",
      "\tG_A, 4.56621\n",
      "\tG_B, 0.63461\n",
      "\tcycle_A, 1.67580\n",
      "\tcycle_B, 3.26020\n",
      "\tidt_A, 1.60088\n",
      "\tidt_B, 0.83342\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "63 / 200\n",
      "\tD_A, 0.03650\n",
      "\tD_B, 0.58044\n",
      "\tG_A, 52.26867\n",
      "\tG_B, 0.93258\n",
      "\tcycle_A, 2.97502\n",
      "\tcycle_B, 3.42684\n",
      "\tidt_A, 1.67824\n",
      "\tidt_B, 1.47359\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "64 / 200\n",
      "\tD_A, 0.61480\n",
      "\tD_B, 0.54193\n",
      "\tG_A, 0.48368\n",
      "\tG_B, 0.97184\n",
      "\tcycle_A, 2.26443\n",
      "\tcycle_B, 3.33947\n",
      "\tidt_A, 1.65916\n",
      "\tidt_B, 1.11539\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "65 / 200\n",
      "\tD_A, 0.19032\n",
      "\tD_B, 0.22423\n",
      "\tG_A, 7.98072\n",
      "\tG_B, 2.66846\n",
      "\tcycle_A, 2.02772\n",
      "\tcycle_B, 3.26161\n",
      "\tidt_A, 1.58557\n",
      "\tidt_B, 0.98397\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "66 / 200\n",
      "\tD_A, 0.53599\n",
      "\tD_B, 0.04719\n",
      "\tG_A, 1.09059\n",
      "\tG_B, 6.37549\n",
      "\tcycle_A, 2.58883\n",
      "\tcycle_B, 3.23743\n",
      "\tidt_A, 1.57040\n",
      "\tidt_B, 1.28703\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "67 / 200\n",
      "\tD_A, 0.48767\n",
      "\tD_B, 0.13991\n",
      "\tG_A, 1.14423\n",
      "\tG_B, 3.23320\n",
      "\tcycle_A, 2.64716\n",
      "\tcycle_B, 3.27464\n",
      "\tidt_A, 1.57993\n",
      "\tidt_B, 1.29483\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "68 / 200\n",
      "\tD_A, 1.27764\n",
      "\tD_B, 0.22180\n",
      "\tG_A, 0.14301\n",
      "\tG_B, 4.14966\n",
      "\tcycle_A, 2.61265\n",
      "\tcycle_B, 3.56121\n",
      "\tidt_A, 1.69409\n",
      "\tidt_B, 1.29308\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "69 / 200\n",
      "\tD_A, 0.04873\n",
      "\tD_B, 0.35967\n",
      "\tG_A, 6.77258\n",
      "\tG_B, 2.00497\n",
      "\tcycle_A, 3.61475\n",
      "\tcycle_B, 3.78858\n",
      "\tidt_A, 1.82579\n",
      "\tidt_B, 1.77993\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "70 / 200\n",
      "\tD_A, 0.01959\n",
      "\tD_B, 4.05704\n",
      "\tG_A, 12.58114\n",
      "\tG_B, 0.69450\n",
      "\tcycle_A, 3.43140\n",
      "\tcycle_B, 3.46990\n",
      "\tidt_A, 1.68235\n",
      "\tidt_B, 1.66716\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "71 / 200\n",
      "\tD_A, 0.08478\n",
      "\tD_B, 0.63009\n",
      "\tG_A, 4.70208\n",
      "\tG_B, 1.16743\n",
      "\tcycle_A, 2.57908\n",
      "\tcycle_B, 3.34667\n",
      "\tidt_A, 1.63649\n",
      "\tidt_B, 1.28401\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "72 / 200\n",
      "\tD_A, 0.64951\n",
      "\tD_B, 0.16246\n",
      "\tG_A, 0.93992\n",
      "\tG_B, 3.49183\n",
      "\tcycle_A, 2.11046\n",
      "\tcycle_B, 3.32841\n",
      "\tidt_A, 1.63898\n",
      "\tidt_B, 1.05617\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "73 / 200\n",
      "\tD_A, 0.60019\n",
      "\tD_B, 0.61334\n",
      "\tG_A, 0.80965\n",
      "\tG_B, 0.67008\n",
      "\tcycle_A, 1.96811\n",
      "\tcycle_B, 3.46139\n",
      "\tidt_A, 1.60035\n",
      "\tidt_B, 0.97966\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "74 / 200\n",
      "\tD_A, 0.52593\n",
      "\tD_B, 0.68253\n",
      "\tG_A, 0.99489\n",
      "\tG_B, 0.62728\n",
      "\tcycle_A, 1.90811\n",
      "\tcycle_B, 3.10980\n",
      "\tidt_A, 1.59141\n",
      "\tidt_B, 0.95158\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "75 / 200\n",
      "\tD_A, 0.32873\n",
      "\tD_B, 0.69715\n",
      "\tG_A, 0.74233\n",
      "\tG_B, 0.70444\n",
      "\tcycle_A, 3.17840\n",
      "\tcycle_B, 3.11106\n",
      "\tidt_A, 1.59279\n",
      "\tidt_B, 1.59128\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "76 / 200\n",
      "\tD_A, 0.14624\n",
      "\tD_B, 0.69226\n",
      "\tG_A, 4.18765\n",
      "\tG_B, 0.71158\n",
      "\tcycle_A, 2.20705\n",
      "\tcycle_B, 3.18632\n",
      "\tidt_A, 1.63775\n",
      "\tidt_B, 1.07773\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "77 / 200\n",
      "\tD_A, 0.15438\n",
      "\tD_B, 0.68079\n",
      "\tG_A, 9.06214\n",
      "\tG_B, 0.72231\n",
      "\tcycle_A, 1.95571\n",
      "\tcycle_B, 3.12814\n",
      "\tidt_A, 1.60660\n",
      "\tidt_B, 0.91638\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "78 / 200\n",
      "\tD_A, 0.53431\n",
      "\tD_B, 0.65575\n",
      "\tG_A, 1.07159\n",
      "\tG_B, 0.69481\n",
      "\tcycle_A, 1.55444\n",
      "\tcycle_B, 3.34426\n",
      "\tidt_A, 1.68832\n",
      "\tidt_B, 0.75754\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "79 / 200\n",
      "\tD_A, 0.58373\n",
      "\tD_B, 0.63603\n",
      "\tG_A, 1.15212\n",
      "\tG_B, 0.74451\n",
      "\tcycle_A, 2.08713\n",
      "\tcycle_B, 3.21570\n",
      "\tidt_A, 1.62757\n",
      "\tidt_B, 1.02092\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "80 / 200\n",
      "\tD_A, 0.61152\n",
      "\tD_B, 0.64468\n",
      "\tG_A, 1.42803\n",
      "\tG_B, 0.84273\n",
      "\tcycle_A, 1.56949\n",
      "\tcycle_B, 3.14207\n",
      "\tidt_A, 1.59292\n",
      "\tidt_B, 0.76174\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "81 / 200\n",
      "\tD_A, 0.54116\n",
      "\tD_B, 0.73607\n",
      "\tG_A, 1.00811\n",
      "\tG_B, 0.90578\n",
      "\tcycle_A, 3.00570\n",
      "\tcycle_B, 3.14483\n",
      "\tidt_A, 1.58215\n",
      "\tidt_B, 1.53172\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "82 / 200\n",
      "\tD_A, 0.63249\n",
      "\tD_B, 0.73624\n",
      "\tG_A, 0.76673\n",
      "\tG_B, 0.73284\n",
      "\tcycle_A, 2.30274\n",
      "\tcycle_B, 3.11223\n",
      "\tidt_A, 1.56559\n",
      "\tidt_B, 1.18007\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "83 / 200\n",
      "\tD_A, 0.44698\n",
      "\tD_B, 0.67447\n",
      "\tG_A, 3.89278\n",
      "\tG_B, 0.77962\n",
      "\tcycle_A, 2.04819\n",
      "\tcycle_B, 3.19529\n",
      "\tidt_A, 1.60219\n",
      "\tidt_B, 1.06535\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "84 / 200\n",
      "\tD_A, 0.54820\n",
      "\tD_B, 0.70207\n",
      "\tG_A, 1.52365\n",
      "\tG_B, 0.82414\n",
      "\tcycle_A, 2.15745\n",
      "\tcycle_B, 3.15146\n",
      "\tidt_A, 1.59197\n",
      "\tidt_B, 1.17025\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "85 / 200\n",
      "\tD_A, 0.64840\n",
      "\tD_B, 0.69647\n",
      "\tG_A, 0.73748\n",
      "\tG_B, 0.68290\n",
      "\tcycle_A, 2.46805\n",
      "\tcycle_B, 3.08606\n",
      "\tidt_A, 1.55399\n",
      "\tidt_B, 1.32620\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "86 / 200\n",
      "\tD_A, 0.62104\n",
      "\tD_B, 0.71248\n",
      "\tG_A, 0.86456\n",
      "\tG_B, 0.86364\n",
      "\tcycle_A, 1.72182\n",
      "\tcycle_B, 3.07157\n",
      "\tidt_A, 1.54340\n",
      "\tidt_B, 0.91874\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "87 / 200\n",
      "\tD_A, 0.57286\n",
      "\tD_B, 0.69530\n",
      "\tG_A, 0.90724\n",
      "\tG_B, 0.86394\n",
      "\tcycle_A, 3.81712\n",
      "\tcycle_B, 3.08306\n",
      "\tidt_A, 1.55070\n",
      "\tidt_B, 2.01231\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "88 / 200\n",
      "\tD_A, 0.55978\n",
      "\tD_B, 0.60397\n",
      "\tG_A, 0.96153\n",
      "\tG_B, 0.67227\n",
      "\tcycle_A, 1.54610\n",
      "\tcycle_B, 3.13439\n",
      "\tidt_A, 1.58487\n",
      "\tidt_B, 0.79007\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "89 / 200\n",
      "\tD_A, 1.18841\n",
      "\tD_B, 0.58815\n",
      "\tG_A, 0.63577\n",
      "\tG_B, 0.70204\n",
      "\tcycle_A, 2.13206\n",
      "\tcycle_B, 3.10001\n",
      "\tidt_A, 1.56457\n",
      "\tidt_B, 1.12393\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "90 / 200\n",
      "\tD_A, 0.54160\n",
      "\tD_B, 0.64025\n",
      "\tG_A, 1.62259\n",
      "\tG_B, 0.84557\n",
      "\tcycle_A, 2.11754\n",
      "\tcycle_B, 3.09377\n",
      "\tidt_A, 1.56701\n",
      "\tidt_B, 1.08179\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "91 / 200\n",
      "\tD_A, 0.19791\n",
      "\tD_B, 0.65367\n",
      "\tG_A, 3.88156\n",
      "\tG_B, 0.60731\n",
      "\tcycle_A, 1.58659\n",
      "\tcycle_B, 3.06224\n",
      "\tidt_A, 1.54035\n",
      "\tidt_B, 0.82239\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "92 / 200\n",
      "\tD_A, 0.56841\n",
      "\tD_B, 0.70561\n",
      "\tG_A, 1.81490\n",
      "\tG_B, 0.71098\n",
      "\tcycle_A, 2.97205\n",
      "\tcycle_B, 3.05974\n",
      "\tidt_A, 1.54198\n",
      "\tidt_B, 1.62573\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "93 / 200\n",
      "\tD_A, 0.58697\n",
      "\tD_B, 0.63756\n",
      "\tG_A, 1.76333\n",
      "\tG_B, 0.91385\n",
      "\tcycle_A, 2.02781\n",
      "\tcycle_B, 3.22293\n",
      "\tidt_A, 1.61261\n",
      "\tidt_B, 1.07495\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "94 / 200\n",
      "\tD_A, 0.71018\n",
      "\tD_B, 0.56192\n",
      "\tG_A, 0.56707\n",
      "\tG_B, 0.84785\n",
      "\tcycle_A, 1.55192\n",
      "\tcycle_B, 3.11577\n",
      "\tidt_A, 1.56813\n",
      "\tidt_B, 0.84218\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "95 / 200\n",
      "\tD_A, 0.62273\n",
      "\tD_B, 0.68632\n",
      "\tG_A, 0.67660\n",
      "\tG_B, 0.79707\n",
      "\tcycle_A, 2.29154\n",
      "\tcycle_B, 3.08228\n",
      "\tidt_A, 1.55085\n",
      "\tidt_B, 1.27449\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "96 / 200\n",
      "\tD_A, 0.61216\n",
      "\tD_B, 0.68418\n",
      "\tG_A, 0.82426\n",
      "\tG_B, 0.60809\n",
      "\tcycle_A, 3.30105\n",
      "\tcycle_B, 3.04844\n",
      "\tidt_A, 1.53368\n",
      "\tidt_B, 1.81254\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "97 / 200\n",
      "\tD_A, 0.59688\n",
      "\tD_B, 0.52451\n",
      "\tG_A, 0.63133\n",
      "\tG_B, 0.76963\n",
      "\tcycle_A, 1.80998\n",
      "\tcycle_B, 3.08926\n",
      "\tidt_A, 1.53380\n",
      "\tidt_B, 1.00015\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "98 / 200\n",
      "\tD_A, 0.51172\n",
      "\tD_B, 0.64467\n",
      "\tG_A, 0.95614\n",
      "\tG_B, 0.68357\n",
      "\tcycle_A, 2.22246\n",
      "\tcycle_B, 3.13394\n",
      "\tidt_A, 1.56755\n",
      "\tidt_B, 1.19843\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "99 / 200\n",
      "\tD_A, 0.46256\n",
      "\tD_B, 0.69905\n",
      "\tG_A, 0.89337\n",
      "\tG_B, 1.61933\n",
      "\tcycle_A, 3.04584\n",
      "\tcycle_B, 3.14163\n",
      "\tidt_A, 1.56183\n",
      "\tidt_B, 1.61416\n",
      "learning rate 2.00e-04 -> 2.00e-04\n",
      "\n",
      "100 / 200\n",
      "\tD_A, 0.19780\n",
      "\tD_B, 0.56552\n",
      "\tG_A, 6.69107\n",
      "\tG_B, 0.82555\n",
      "\tcycle_A, 1.41579\n",
      "\tcycle_B, 3.23055\n",
      "\tidt_A, 1.61946\n",
      "\tidt_B, 0.71742\n",
      "learning rate 2.00e-04 -> 1.98e-04\n",
      "\n",
      "101 / 200\n",
      "\tD_A, 0.12719\n",
      "\tD_B, 0.63842\n",
      "\tG_A, 29.16750\n",
      "\tG_B, 0.50131\n",
      "\tcycle_A, 2.06003\n",
      "\tcycle_B, 3.34289\n",
      "\tidt_A, 1.66934\n",
      "\tidt_B, 1.03913\n",
      "learning rate 1.98e-04 -> 1.96e-04\n",
      "\n",
      "102 / 200\n",
      "\tD_A, 0.35910\n",
      "\tD_B, 0.60005\n",
      "\tG_A, 2.15150\n",
      "\tG_B, 0.70952\n",
      "\tcycle_A, 2.18741\n",
      "\tcycle_B, 3.14882\n",
      "\tidt_A, 1.56789\n",
      "\tidt_B, 1.12832\n",
      "learning rate 1.96e-04 -> 1.94e-04\n",
      "\n",
      "103 / 200\n",
      "\tD_A, 0.23915\n",
      "\tD_B, 0.62502\n",
      "\tG_A, 4.00155\n",
      "\tG_B, 0.94570\n",
      "\tcycle_A, 1.56059\n",
      "\tcycle_B, 3.18093\n",
      "\tidt_A, 1.59516\n",
      "\tidt_B, 0.80402\n",
      "learning rate 1.94e-04 -> 1.92e-04\n",
      "\n",
      "104 / 200\n",
      "\tD_A, 0.16485\n",
      "\tD_B, 0.65663\n",
      "\tG_A, 4.79018\n",
      "\tG_B, 0.84270\n",
      "\tcycle_A, 2.00358\n",
      "\tcycle_B, 3.09585\n",
      "\tidt_A, 1.54774\n",
      "\tidt_B, 1.10311\n",
      "learning rate 1.92e-04 -> 1.90e-04\n",
      "\n",
      "105 / 200\n",
      "\tD_A, 0.49630\n",
      "\tD_B, 0.59641\n",
      "\tG_A, 1.62567\n",
      "\tG_B, 0.91110\n",
      "\tcycle_A, 2.50058\n",
      "\tcycle_B, 3.14926\n",
      "\tidt_A, 1.58040\n",
      "\tidt_B, 1.32334\n",
      "learning rate 1.90e-04 -> 1.88e-04\n",
      "\n",
      "106 / 200\n",
      "\tD_A, 0.12195\n",
      "\tD_B, 0.41066\n",
      "\tG_A, 13.71965\n",
      "\tG_B, 1.22651\n",
      "\tcycle_A, 1.45483\n",
      "\tcycle_B, 3.13413\n",
      "\tidt_A, 1.56437\n",
      "\tidt_B, 0.74333\n",
      "learning rate 1.88e-04 -> 1.86e-04\n",
      "\n",
      "107 / 200\n",
      "\tD_A, 0.25568\n",
      "\tD_B, 0.65347\n",
      "\tG_A, 4.73203\n",
      "\tG_B, 1.70917\n",
      "\tcycle_A, 2.28055\n",
      "\tcycle_B, 3.13133\n",
      "\tidt_A, 1.55641\n",
      "\tidt_B, 1.18880\n",
      "learning rate 1.86e-04 -> 1.84e-04\n",
      "\n",
      "108 / 200\n",
      "\tD_A, 0.15363\n",
      "\tD_B, 0.47344\n",
      "\tG_A, 6.00104\n",
      "\tG_B, 0.80113\n",
      "\tcycle_A, 1.58459\n",
      "\tcycle_B, 3.10744\n",
      "\tidt_A, 1.55110\n",
      "\tidt_B, 0.82252\n",
      "learning rate 1.84e-04 -> 1.82e-04\n",
      "\n",
      "109 / 200\n",
      "\tD_A, 0.00989\n",
      "\tD_B, 0.41498\n",
      "\tG_A, 34.31537\n",
      "\tG_B, 0.95606\n",
      "\tcycle_A, 2.10459\n",
      "\tcycle_B, 3.15022\n",
      "\tidt_A, 1.57327\n",
      "\tidt_B, 1.08726\n",
      "learning rate 1.82e-04 -> 1.80e-04\n",
      "\n",
      "110 / 200\n",
      "\tD_A, 0.06113\n",
      "\tD_B, 0.49133\n",
      "\tG_A, 7.26361\n",
      "\tG_B, 0.80019\n",
      "\tcycle_A, 1.65222\n",
      "\tcycle_B, 3.12327\n",
      "\tidt_A, 1.55515\n",
      "\tidt_B, 0.87266\n",
      "learning rate 1.80e-04 -> 1.78e-04\n",
      "\n",
      "111 / 200\n",
      "\tD_A, 0.09954\n",
      "\tD_B, 0.41610\n",
      "\tG_A, 6.81051\n",
      "\tG_B, 0.75074\n",
      "\tcycle_A, 1.84098\n",
      "\tcycle_B, 3.11023\n",
      "\tidt_A, 1.53884\n",
      "\tidt_B, 0.91333\n",
      "learning rate 1.78e-04 -> 1.76e-04\n",
      "\n",
      "112 / 200\n",
      "\tD_A, 0.07094\n",
      "\tD_B, 0.61739\n",
      "\tG_A, 7.76861\n",
      "\tG_B, 0.83584\n",
      "\tcycle_A, 3.69699\n",
      "\tcycle_B, 3.10059\n",
      "\tidt_A, 1.53939\n",
      "\tidt_B, 1.87349\n",
      "learning rate 1.76e-04 -> 1.74e-04\n",
      "\n",
      "113 / 200\n",
      "\tD_A, 0.28761\n",
      "\tD_B, 0.51107\n",
      "\tG_A, 1.39120\n",
      "\tG_B, 0.90986\n",
      "\tcycle_A, 2.06522\n",
      "\tcycle_B, 3.12431\n",
      "\tidt_A, 1.55159\n",
      "\tidt_B, 1.10236\n",
      "learning rate 1.74e-04 -> 1.72e-04\n",
      "\n",
      "114 / 200\n",
      "\tD_A, 0.54276\n",
      "\tD_B, 0.62572\n",
      "\tG_A, 4.50515\n",
      "\tG_B, 1.92736\n",
      "\tcycle_A, 2.56521\n",
      "\tcycle_B, 3.18319\n",
      "\tidt_A, 1.58081\n",
      "\tidt_B, 1.29758\n",
      "learning rate 1.72e-04 -> 1.70e-04\n",
      "\n",
      "115 / 200\n",
      "\tD_A, 0.58340\n",
      "\tD_B, 1.13427\n",
      "\tG_A, 0.97789\n",
      "\tG_B, 1.07307\n",
      "\tcycle_A, 1.55470\n",
      "\tcycle_B, 3.19296\n",
      "\tidt_A, 1.57419\n",
      "\tidt_B, 0.77763\n",
      "learning rate 1.70e-04 -> 1.68e-04\n",
      "\n",
      "116 / 200\n",
      "\tD_A, 0.54362\n",
      "\tD_B, 0.51211\n",
      "\tG_A, 1.14073\n",
      "\tG_B, 1.00863\n",
      "\tcycle_A, 2.63376\n",
      "\tcycle_B, 3.18439\n",
      "\tidt_A, 1.58564\n",
      "\tidt_B, 1.39244\n",
      "learning rate 1.68e-04 -> 1.66e-04\n",
      "\n",
      "117 / 200\n",
      "\tD_A, 0.41213\n",
      "\tD_B, 0.28225\n",
      "\tG_A, 3.28835\n",
      "\tG_B, 1.81854\n",
      "\tcycle_A, 1.26191\n",
      "\tcycle_B, 3.13621\n",
      "\tidt_A, 1.55685\n",
      "\tidt_B, 0.66044\n",
      "learning rate 1.66e-04 -> 1.64e-04\n",
      "\n",
      "118 / 200\n",
      "\tD_A, 0.51396\n",
      "\tD_B, 0.09412\n",
      "\tG_A, 0.96730\n",
      "\tG_B, 4.28962\n",
      "\tcycle_A, 2.08126\n",
      "\tcycle_B, 3.15795\n",
      "\tidt_A, 1.56045\n",
      "\tidt_B, 1.04571\n",
      "learning rate 1.64e-04 -> 1.62e-04\n",
      "\n",
      "119 / 200\n",
      "\tD_A, 1.02558\n",
      "\tD_B, 0.34975\n",
      "\tG_A, 0.20668\n",
      "\tG_B, 3.85835\n",
      "\tcycle_A, 2.47438\n",
      "\tcycle_B, 3.17708\n",
      "\tidt_A, 1.56540\n",
      "\tidt_B, 1.23725\n",
      "learning rate 1.62e-04 -> 1.60e-04\n",
      "\n",
      "120 / 200\n",
      "\tD_A, 0.18236\n",
      "\tD_B, 0.22992\n",
      "\tG_A, 2.59704\n",
      "\tG_B, 2.53856\n",
      "\tcycle_A, 2.05283\n",
      "\tcycle_B, 3.14042\n",
      "\tidt_A, 1.55028\n",
      "\tidt_B, 1.05431\n",
      "learning rate 1.60e-04 -> 1.58e-04\n",
      "\n",
      "121 / 200\n",
      "\tD_A, 0.24269\n",
      "\tD_B, 0.35736\n",
      "\tG_A, 2.48002\n",
      "\tG_B, 2.22561\n",
      "\tcycle_A, 4.08906\n",
      "\tcycle_B, 3.15541\n",
      "\tidt_A, 1.56004\n",
      "\tidt_B, 2.15610\n",
      "learning rate 1.58e-04 -> 1.56e-04\n",
      "\n",
      "122 / 200\n",
      "\tD_A, 0.20265\n",
      "\tD_B, 0.22322\n",
      "\tG_A, 2.93525\n",
      "\tG_B, 1.81125\n",
      "\tcycle_A, 2.34925\n",
      "\tcycle_B, 3.15470\n",
      "\tidt_A, 1.56286\n",
      "\tidt_B, 1.17260\n",
      "learning rate 1.56e-04 -> 1.54e-04\n",
      "\n",
      "123 / 200\n",
      "\tD_A, 0.58069\n",
      "\tD_B, 0.40402\n",
      "\tG_A, 1.12663\n",
      "\tG_B, 1.95990\n",
      "\tcycle_A, 3.43401\n",
      "\tcycle_B, 3.17629\n",
      "\tidt_A, 1.57346\n",
      "\tidt_B, 1.75885\n",
      "learning rate 1.54e-04 -> 1.52e-04\n",
      "\n",
      "124 / 200\n",
      "\tD_A, 0.50114\n",
      "\tD_B, 0.19048\n",
      "\tG_A, 1.67025\n",
      "\tG_B, 3.89859\n",
      "\tcycle_A, 1.97430\n",
      "\tcycle_B, 3.18291\n",
      "\tidt_A, 1.57998\n",
      "\tidt_B, 1.01228\n",
      "learning rate 1.52e-04 -> 1.50e-04\n",
      "\n",
      "125 / 200\n",
      "\tD_A, 0.36228\n",
      "\tD_B, 0.20362\n",
      "\tG_A, 2.42414\n",
      "\tG_B, 3.66675\n",
      "\tcycle_A, 2.99018\n",
      "\tcycle_B, 3.14750\n",
      "\tidt_A, 1.56000\n",
      "\tidt_B, 1.50548\n",
      "learning rate 1.50e-04 -> 1.49e-04\n",
      "\n",
      "126 / 200\n",
      "\tD_A, 0.57427\n",
      "\tD_B, 0.08011\n",
      "\tG_A, 1.77327\n",
      "\tG_B, 5.15716\n",
      "\tcycle_A, 2.26105\n",
      "\tcycle_B, 3.14672\n",
      "\tidt_A, 1.56260\n",
      "\tidt_B, 1.12167\n",
      "learning rate 1.49e-04 -> 1.47e-04\n",
      "\n",
      "127 / 200\n",
      "\tD_A, 0.84815\n",
      "\tD_B, 0.54999\n",
      "\tG_A, 0.31161\n",
      "\tG_B, 0.72295\n",
      "\tcycle_A, 1.50591\n",
      "\tcycle_B, 3.17794\n",
      "\tidt_A, 1.57426\n",
      "\tidt_B, 0.73721\n",
      "learning rate 1.47e-04 -> 1.45e-04\n",
      "\n",
      "128 / 200\n",
      "\tD_A, 0.52585\n",
      "\tD_B, 0.17508\n",
      "\tG_A, 1.47312\n",
      "\tG_B, 1.93249\n",
      "\tcycle_A, 1.79988\n",
      "\tcycle_B, 3.17931\n",
      "\tidt_A, 1.57810\n",
      "\tidt_B, 0.89300\n",
      "learning rate 1.45e-04 -> 1.43e-04\n",
      "\n",
      "129 / 200\n",
      "\tD_A, 0.32380\n",
      "\tD_B, 0.17643\n",
      "\tG_A, 2.87429\n",
      "\tG_B, 3.39799\n",
      "\tcycle_A, 1.84905\n",
      "\tcycle_B, 3.19891\n",
      "\tidt_A, 1.58749\n",
      "\tidt_B, 0.91068\n",
      "learning rate 1.43e-04 -> 1.41e-04\n",
      "\n",
      "130 / 200\n",
      "\tD_A, 0.54254\n",
      "\tD_B, 0.11914\n",
      "\tG_A, 0.69883\n",
      "\tG_B, 3.77575\n",
      "\tcycle_A, 3.11134\n",
      "\tcycle_B, 3.15902\n",
      "\tidt_A, 1.56963\n",
      "\tidt_B, 1.55473\n",
      "learning rate 1.41e-04 -> 1.39e-04\n",
      "\n",
      "131 / 200\n",
      "\tD_A, 0.40207\n",
      "\tD_B, 0.07970\n",
      "\tG_A, 2.36684\n",
      "\tG_B, 3.89462\n",
      "\tcycle_A, 1.44280\n",
      "\tcycle_B, 3.19315\n",
      "\tidt_A, 1.58637\n",
      "\tidt_B, 0.71980\n",
      "learning rate 1.39e-04 -> 1.37e-04\n",
      "\n",
      "132 / 200\n",
      "\tD_A, 0.21288\n",
      "\tD_B, 0.12826\n",
      "\tG_A, 4.11880\n",
      "\tG_B, 5.15691\n",
      "\tcycle_A, 1.62742\n",
      "\tcycle_B, 3.14584\n",
      "\tidt_A, 1.55871\n",
      "\tidt_B, 0.80697\n",
      "learning rate 1.37e-04 -> 1.35e-04\n",
      "\n",
      "133 / 200\n",
      "\tD_A, 0.81991\n",
      "\tD_B, 0.30547\n",
      "\tG_A, 0.44275\n",
      "\tG_B, 1.89526\n",
      "\tcycle_A, 2.13530\n",
      "\tcycle_B, 3.17616\n",
      "\tidt_A, 1.57348\n",
      "\tidt_B, 1.07036\n",
      "learning rate 1.35e-04 -> 1.33e-04\n",
      "\n",
      "134 / 200\n",
      "\tD_A, 0.15259\n",
      "\tD_B, 0.24566\n",
      "\tG_A, 3.76915\n",
      "\tG_B, 2.53699\n",
      "\tcycle_A, 2.62562\n",
      "\tcycle_B, 3.15931\n",
      "\tidt_A, 1.56571\n",
      "\tidt_B, 1.29613\n",
      "learning rate 1.33e-04 -> 1.31e-04\n",
      "\n",
      "135 / 200\n",
      "\tD_A, 0.32528\n",
      "\tD_B, 0.24824\n",
      "\tG_A, 2.54006\n",
      "\tG_B, 4.86216\n",
      "\tcycle_A, 2.83784\n",
      "\tcycle_B, 3.22935\n",
      "\tidt_A, 1.59932\n",
      "\tidt_B, 1.43492\n",
      "learning rate 1.31e-04 -> 1.29e-04\n",
      "\n",
      "136 / 200\n",
      "\tD_A, 0.20446\n",
      "\tD_B, 0.25774\n",
      "\tG_A, 5.06114\n",
      "\tG_B, 4.90369\n",
      "\tcycle_A, 2.65321\n",
      "\tcycle_B, 3.14477\n",
      "\tidt_A, 1.55374\n",
      "\tidt_B, 1.32713\n",
      "learning rate 1.29e-04 -> 1.27e-04\n",
      "\n",
      "137 / 200\n",
      "\tD_A, 0.34060\n",
      "\tD_B, 0.07529\n",
      "\tG_A, 2.96203\n",
      "\tG_B, 4.23063\n",
      "\tcycle_A, 2.95974\n",
      "\tcycle_B, 3.19502\n",
      "\tidt_A, 1.58426\n",
      "\tidt_B, 1.47699\n",
      "learning rate 1.27e-04 -> 1.25e-04\n",
      "\n",
      "138 / 200\n",
      "\tD_A, 0.50607\n",
      "\tD_B, 0.16428\n",
      "\tG_A, 1.27517\n",
      "\tG_B, 6.15117\n",
      "\tcycle_A, 2.08096\n",
      "\tcycle_B, 3.19089\n",
      "\tidt_A, 1.58513\n",
      "\tidt_B, 1.04176\n",
      "learning rate 1.25e-04 -> 1.23e-04\n",
      "\n",
      "139 / 200\n",
      "\tD_A, 0.49764\n",
      "\tD_B, 0.33080\n",
      "\tG_A, 1.62801\n",
      "\tG_B, 5.12854\n",
      "\tcycle_A, 2.77375\n",
      "\tcycle_B, 3.19936\n",
      "\tidt_A, 1.58542\n",
      "\tidt_B, 1.38968\n",
      "learning rate 1.23e-04 -> 1.21e-04\n",
      "\n",
      "140 / 200\n",
      "\tD_A, 0.49165\n",
      "\tD_B, 0.15214\n",
      "\tG_A, 1.16975\n",
      "\tG_B, 3.35904\n",
      "\tcycle_A, 2.19141\n",
      "\tcycle_B, 3.16993\n",
      "\tidt_A, 1.56895\n",
      "\tidt_B, 1.09881\n",
      "learning rate 1.21e-04 -> 1.19e-04\n",
      "\n",
      "141 / 200\n",
      "\tD_A, 0.43391\n",
      "\tD_B, 0.14773\n",
      "\tG_A, 1.22108\n",
      "\tG_B, 3.91561\n",
      "\tcycle_A, 1.67650\n",
      "\tcycle_B, 3.19104\n",
      "\tidt_A, 1.58336\n",
      "\tidt_B, 0.84339\n",
      "learning rate 1.19e-04 -> 1.17e-04\n",
      "\n",
      "142 / 200\n",
      "\tD_A, 0.46571\n",
      "\tD_B, 0.19893\n",
      "\tG_A, 109.48776\n",
      "\tG_B, 3.56340\n",
      "\tcycle_A, 2.02417\n",
      "\tcycle_B, 3.48337\n",
      "\tidt_A, 1.72714\n",
      "\tidt_B, 0.98399\n",
      "learning rate 1.17e-04 -> 1.15e-04\n",
      "\n",
      "143 / 200\n",
      "\tD_A, 0.06889\n",
      "\tD_B, 0.23032\n",
      "\tG_A, 6.78701\n",
      "\tG_B, 1.64179\n",
      "\tcycle_A, 1.90531\n",
      "\tcycle_B, 3.13941\n",
      "\tidt_A, 1.55649\n",
      "\tidt_B, 0.94571\n",
      "learning rate 1.15e-04 -> 1.13e-04\n",
      "\n",
      "144 / 200\n",
      "\tD_A, 0.30541\n",
      "\tD_B, 0.13200\n",
      "\tG_A, 1.30074\n",
      "\tG_B, 3.02276\n",
      "\tcycle_A, 2.17608\n",
      "\tcycle_B, 3.19129\n",
      "\tidt_A, 1.58665\n",
      "\tidt_B, 1.09664\n",
      "learning rate 1.13e-04 -> 1.11e-04\n",
      "\n",
      "145 / 200\n",
      "\tD_A, 0.28853\n",
      "\tD_B, 0.29760\n",
      "\tG_A, 3.17272\n",
      "\tG_B, 1.71850\n",
      "\tcycle_A, 1.32893\n",
      "\tcycle_B, 3.15109\n",
      "\tidt_A, 1.56699\n",
      "\tidt_B, 0.65892\n",
      "learning rate 1.11e-04 -> 1.09e-04\n",
      "\n",
      "146 / 200\n",
      "\tD_A, 0.29328\n",
      "\tD_B, 0.36008\n",
      "\tG_A, 4.34438\n",
      "\tG_B, 2.90917\n",
      "\tcycle_A, 2.26329\n",
      "\tcycle_B, 3.19142\n",
      "\tidt_A, 1.58772\n",
      "\tidt_B, 1.13729\n",
      "learning rate 1.09e-04 -> 1.07e-04\n",
      "\n",
      "147 / 200\n",
      "\tD_A, 0.30030\n",
      "\tD_B, 0.13684\n",
      "\tG_A, 1.76055\n",
      "\tG_B, 3.14190\n",
      "\tcycle_A, 1.77276\n",
      "\tcycle_B, 3.20541\n",
      "\tidt_A, 1.59228\n",
      "\tidt_B, 0.89143\n",
      "learning rate 1.07e-04 -> 1.05e-04\n",
      "\n",
      "148 / 200\n",
      "\tD_A, 0.66252\n",
      "\tD_B, 0.16553\n",
      "\tG_A, 0.60611\n",
      "\tG_B, 2.94943\n",
      "\tcycle_A, 2.25049\n",
      "\tcycle_B, 3.19488\n",
      "\tidt_A, 1.58500\n",
      "\tidt_B, 1.13031\n",
      "learning rate 1.05e-04 -> 1.03e-04\n",
      "\n",
      "149 / 200\n",
      "\tD_A, 0.27509\n",
      "\tD_B, 0.28408\n",
      "\tG_A, 4.03750\n",
      "\tG_B, 4.27655\n",
      "\tcycle_A, 1.97659\n",
      "\tcycle_B, 3.22192\n",
      "\tidt_A, 1.59581\n",
      "\tidt_B, 0.97551\n",
      "learning rate 1.03e-04 -> 1.01e-04\n",
      "\n",
      "150 / 200\n",
      "\tD_A, 0.55495\n",
      "\tD_B, 0.22557\n",
      "\tG_A, 1.36589\n",
      "\tG_B, 4.21061\n",
      "\tcycle_A, 2.64250\n",
      "\tcycle_B, 3.20793\n",
      "\tidt_A, 1.59338\n",
      "\tidt_B, 1.33003\n",
      "learning rate 1.01e-04 -> 9.90e-05\n",
      "\n",
      "151 / 200\n",
      "\tD_A, 0.73865\n",
      "\tD_B, 1.03450\n",
      "\tG_A, 2.97669\n",
      "\tG_B, 5.68882\n",
      "\tcycle_A, 1.29809\n",
      "\tcycle_B, 3.20796\n",
      "\tidt_A, 1.59349\n",
      "\tidt_B, 0.65425\n",
      "learning rate 9.90e-05 -> 9.70e-05\n",
      "\n",
      "152 / 200\n",
      "\tD_A, 0.05191\n",
      "\tD_B, 0.36700\n",
      "\tG_A, 7.62321\n",
      "\tG_B, 1.12726\n",
      "\tcycle_A, 2.82487\n",
      "\tcycle_B, 3.17840\n",
      "\tidt_A, 1.58178\n",
      "\tidt_B, 1.40333\n",
      "learning rate 9.70e-05 -> 9.50e-05\n",
      "\n",
      "153 / 200\n",
      "\tD_A, 0.39964\n",
      "\tD_B, 0.38818\n",
      "\tG_A, 2.25972\n",
      "\tG_B, 2.42098\n",
      "\tcycle_A, 2.62954\n",
      "\tcycle_B, 3.21092\n",
      "\tidt_A, 1.59713\n",
      "\tidt_B, 1.31601\n",
      "learning rate 9.50e-05 -> 9.31e-05\n",
      "\n",
      "154 / 200\n",
      "\tD_A, 0.30063\n",
      "\tD_B, 0.30174\n",
      "\tG_A, 3.89560\n",
      "\tG_B, 1.54309\n",
      "\tcycle_A, 3.58553\n",
      "\tcycle_B, 3.22213\n",
      "\tidt_A, 1.60106\n",
      "\tidt_B, 1.80661\n",
      "learning rate 9.31e-05 -> 9.11e-05\n",
      "\n",
      "155 / 200\n",
      "\tD_A, 0.25342\n",
      "\tD_B, 0.13672\n",
      "\tG_A, 9.13467\n",
      "\tG_B, 3.85526\n",
      "\tcycle_A, 1.63720\n",
      "\tcycle_B, 3.20789\n",
      "\tidt_A, 1.59660\n",
      "\tidt_B, 0.81445\n",
      "learning rate 9.11e-05 -> 8.91e-05\n",
      "\n",
      "156 / 200\n",
      "\tD_A, 0.47850\n",
      "\tD_B, 0.42516\n",
      "\tG_A, 2.30993\n",
      "\tG_B, 1.99299\n",
      "\tcycle_A, 4.03177\n",
      "\tcycle_B, 3.18466\n",
      "\tidt_A, 1.58262\n",
      "\tidt_B, 2.01343\n",
      "learning rate 8.91e-05 -> 8.71e-05\n",
      "\n",
      "157 / 200\n",
      "\tD_A, 0.18535\n",
      "\tD_B, 0.25864\n",
      "\tG_A, 5.93876\n",
      "\tG_B, 1.60931\n",
      "\tcycle_A, 1.69403\n",
      "\tcycle_B, 3.20592\n",
      "\tidt_A, 1.59585\n",
      "\tidt_B, 0.85781\n",
      "learning rate 8.71e-05 -> 8.51e-05\n",
      "\n",
      "158 / 200\n",
      "\tD_A, 0.40113\n",
      "\tD_B, 0.29113\n",
      "\tG_A, 4.19130\n",
      "\tG_B, 3.82340\n",
      "\tcycle_A, 2.24990\n",
      "\tcycle_B, 3.21720\n",
      "\tidt_A, 1.59542\n",
      "\tidt_B, 1.14376\n",
      "learning rate 8.51e-05 -> 8.32e-05\n",
      "\n",
      "159 / 200\n",
      "\tD_A, 0.31170\n",
      "\tD_B, 0.19488\n",
      "\tG_A, 3.70624\n",
      "\tG_B, 3.05389\n",
      "\tcycle_A, 1.79534\n",
      "\tcycle_B, 3.17240\n",
      "\tidt_A, 1.56711\n",
      "\tidt_B, 0.89886\n",
      "learning rate 8.32e-05 -> 8.12e-05\n",
      "\n",
      "160 / 200\n",
      "\tD_A, 0.39548\n",
      "\tD_B, 0.18975\n",
      "\tG_A, 2.95585\n",
      "\tG_B, 2.35382\n",
      "\tcycle_A, 4.10981\n",
      "\tcycle_B, 3.21765\n",
      "\tidt_A, 1.59421\n",
      "\tidt_B, 2.06998\n",
      "learning rate 8.12e-05 -> 7.92e-05\n",
      "\n",
      "161 / 200\n",
      "\tD_A, 0.15429\n",
      "\tD_B, 1.10117\n",
      "\tG_A, 36.13361\n",
      "\tG_B, 4.92908\n",
      "\tcycle_A, 1.27257\n",
      "\tcycle_B, 3.22099\n",
      "\tidt_A, 1.59251\n",
      "\tidt_B, 0.64066\n",
      "learning rate 7.92e-05 -> 7.72e-05\n",
      "\n",
      "162 / 200\n",
      "\tD_A, 0.05745\n",
      "\tD_B, 0.28159\n",
      "\tG_A, 5.82545\n",
      "\tG_B, 3.51595\n",
      "\tcycle_A, 2.70351\n",
      "\tcycle_B, 3.22559\n",
      "\tidt_A, 1.59027\n",
      "\tidt_B, 1.36063\n",
      "learning rate 7.72e-05 -> 7.52e-05\n",
      "\n",
      "163 / 200\n",
      "\tD_A, 0.14002\n",
      "\tD_B, 0.61167\n",
      "\tG_A, 4.02377\n",
      "\tG_B, 4.58579\n",
      "\tcycle_A, 1.96069\n",
      "\tcycle_B, 3.21068\n",
      "\tidt_A, 1.59180\n",
      "\tidt_B, 0.99094\n",
      "learning rate 7.52e-05 -> 7.33e-05\n",
      "\n",
      "164 / 200\n",
      "\tD_A, 0.15473\n",
      "\tD_B, 0.46494\n",
      "\tG_A, 6.43142\n",
      "\tG_B, 1.78807\n",
      "\tcycle_A, 2.69312\n",
      "\tcycle_B, 3.20314\n",
      "\tidt_A, 1.59018\n",
      "\tidt_B, 1.33517\n",
      "learning rate 7.33e-05 -> 7.13e-05\n",
      "\n",
      "165 / 200\n",
      "\tD_A, 0.20526\n",
      "\tD_B, 0.21554\n",
      "\tG_A, 9.21346\n",
      "\tG_B, 2.70764\n",
      "\tcycle_A, 2.24402\n",
      "\tcycle_B, 3.20829\n",
      "\tidt_A, 1.59269\n",
      "\tidt_B, 1.13309\n",
      "learning rate 7.13e-05 -> 6.93e-05\n",
      "\n",
      "166 / 200\n",
      "\tD_A, 0.20961\n",
      "\tD_B, 1.14925\n",
      "\tG_A, 5.88562\n",
      "\tG_B, 1.98057\n",
      "\tcycle_A, 3.39251\n",
      "\tcycle_B, 3.20572\n",
      "\tidt_A, 1.59640\n",
      "\tidt_B, 1.69443\n",
      "learning rate 6.93e-05 -> 6.73e-05\n",
      "\n",
      "167 / 200\n",
      "\tD_A, 0.24734\n",
      "\tD_B, 0.36521\n",
      "\tG_A, 5.78843\n",
      "\tG_B, 1.29365\n",
      "\tcycle_A, 2.20099\n",
      "\tcycle_B, 3.21960\n",
      "\tidt_A, 1.59841\n",
      "\tidt_B, 1.10223\n",
      "learning rate 6.73e-05 -> 6.53e-05\n",
      "\n",
      "168 / 200\n",
      "\tD_A, 0.23977\n",
      "\tD_B, 0.42462\n",
      "\tG_A, 2.08737\n",
      "\tG_B, 0.76484\n",
      "\tcycle_A, 2.85956\n",
      "\tcycle_B, 3.21697\n",
      "\tidt_A, 1.60196\n",
      "\tidt_B, 1.44875\n",
      "learning rate 6.53e-05 -> 6.34e-05\n",
      "\n",
      "169 / 200\n",
      "\tD_A, 0.11209\n",
      "\tD_B, 0.35526\n",
      "\tG_A, 3.39068\n",
      "\tG_B, 1.34776\n",
      "\tcycle_A, 2.30065\n",
      "\tcycle_B, 3.21583\n",
      "\tidt_A, 1.60167\n",
      "\tidt_B, 1.14964\n",
      "learning rate 6.34e-05 -> 6.14e-05\n",
      "\n",
      "170 / 200\n",
      "\tD_A, 0.13405\n",
      "\tD_B, 0.28586\n",
      "\tG_A, 6.58981\n",
      "\tG_B, 0.54240\n",
      "\tcycle_A, 1.73611\n",
      "\tcycle_B, 3.22161\n",
      "\tidt_A, 1.60134\n",
      "\tidt_B, 0.85513\n",
      "learning rate 6.14e-05 -> 5.94e-05\n",
      "\n",
      "171 / 200\n",
      "\tD_A, 0.24393\n",
      "\tD_B, 0.54495\n",
      "\tG_A, 3.33104\n",
      "\tG_B, 0.64775\n",
      "\tcycle_A, 3.49255\n",
      "\tcycle_B, 3.21532\n",
      "\tidt_A, 1.59951\n",
      "\tidt_B, 1.76700\n",
      "learning rate 5.94e-05 -> 5.74e-05\n",
      "\n",
      "172 / 200\n",
      "\tD_A, 0.41915\n",
      "\tD_B, 0.27437\n",
      "\tG_A, 8.81906\n",
      "\tG_B, 1.53945\n",
      "\tcycle_A, 1.95925\n",
      "\tcycle_B, 3.22159\n",
      "\tidt_A, 1.60398\n",
      "\tidt_B, 0.99213\n",
      "learning rate 5.74e-05 -> 5.54e-05\n",
      "\n",
      "173 / 200\n",
      "\tD_A, 0.09692\n",
      "\tD_B, 2.83397\n",
      "\tG_A, 5.06679\n",
      "\tG_B, 2.08709\n",
      "\tcycle_A, 2.49764\n",
      "\tcycle_B, 3.18545\n",
      "\tidt_A, 1.58414\n",
      "\tidt_B, 1.25974\n",
      "learning rate 5.54e-05 -> 5.35e-05\n",
      "\n",
      "174 / 200\n",
      "\tD_A, 0.15743\n",
      "\tD_B, 0.67696\n",
      "\tG_A, 3.33977\n",
      "\tG_B, 0.65050\n",
      "\tcycle_A, 3.57329\n",
      "\tcycle_B, 3.20559\n",
      "\tidt_A, 1.60084\n",
      "\tidt_B, 1.79959\n",
      "learning rate 5.35e-05 -> 5.15e-05\n",
      "\n",
      "175 / 200\n",
      "\tD_A, 0.19341\n",
      "\tD_B, 0.91218\n",
      "\tG_A, 7.36404\n",
      "\tG_B, 0.89574\n",
      "\tcycle_A, 2.91608\n",
      "\tcycle_B, 3.21135\n",
      "\tidt_A, 1.60572\n",
      "\tidt_B, 1.46858\n",
      "learning rate 5.15e-05 -> 4.95e-05\n",
      "\n",
      "176 / 200\n",
      "\tD_A, 0.11567\n",
      "\tD_B, 0.41400\n",
      "\tG_A, 4.42825\n",
      "\tG_B, 1.90582\n",
      "\tcycle_A, 3.26597\n",
      "\tcycle_B, 3.22310\n",
      "\tidt_A, 1.60609\n",
      "\tidt_B, 1.63321\n",
      "learning rate 4.95e-05 -> 4.75e-05\n",
      "\n",
      "177 / 200\n",
      "\tD_A, 0.11900\n",
      "\tD_B, 0.76047\n",
      "\tG_A, 4.84933\n",
      "\tG_B, 0.95974\n",
      "\tcycle_A, 1.76968\n",
      "\tcycle_B, 3.21070\n",
      "\tidt_A, 1.60293\n",
      "\tidt_B, 0.87706\n",
      "learning rate 4.75e-05 -> 4.55e-05\n",
      "\n",
      "178 / 200\n",
      "\tD_A, 0.19789\n",
      "\tD_B, 0.45643\n",
      "\tG_A, 3.86841\n",
      "\tG_B, 1.12211\n",
      "\tcycle_A, 2.52858\n",
      "\tcycle_B, 3.21834\n",
      "\tidt_A, 1.60596\n",
      "\tidt_B, 1.29285\n",
      "learning rate 4.55e-05 -> 4.36e-05\n",
      "\n",
      "179 / 200\n",
      "\tD_A, 0.09804\n",
      "\tD_B, 0.33860\n",
      "\tG_A, 4.08199\n",
      "\tG_B, 2.09664\n",
      "\tcycle_A, 2.00904\n",
      "\tcycle_B, 3.22162\n",
      "\tidt_A, 1.60617\n",
      "\tidt_B, 1.00658\n",
      "learning rate 4.36e-05 -> 4.16e-05\n",
      "\n",
      "180 / 200\n",
      "\tD_A, 0.10089\n",
      "\tD_B, 0.58606\n",
      "\tG_A, 5.93904\n",
      "\tG_B, 0.88280\n",
      "\tcycle_A, 2.65107\n",
      "\tcycle_B, 3.21723\n",
      "\tidt_A, 1.60586\n",
      "\tidt_B, 1.36855\n",
      "learning rate 4.16e-05 -> 3.96e-05\n",
      "\n",
      "181 / 200\n",
      "\tD_A, 0.10281\n",
      "\tD_B, 0.54823\n",
      "\tG_A, 5.71143\n",
      "\tG_B, 0.72333\n",
      "\tcycle_A, 2.95167\n",
      "\tcycle_B, 3.22526\n",
      "\tidt_A, 1.61036\n",
      "\tidt_B, 1.52029\n",
      "learning rate 3.96e-05 -> 3.76e-05\n",
      "\n",
      "182 / 200\n",
      "\tD_A, 0.10093\n",
      "\tD_B, 0.61605\n",
      "\tG_A, 5.77955\n",
      "\tG_B, 1.01038\n",
      "\tcycle_A, 1.99345\n",
      "\tcycle_B, 3.22040\n",
      "\tidt_A, 1.60720\n",
      "\tidt_B, 0.99203\n",
      "learning rate 3.76e-05 -> 3.56e-05\n",
      "\n",
      "183 / 200\n",
      "\tD_A, 0.09482\n",
      "\tD_B, 0.34922\n",
      "\tG_A, 7.82356\n",
      "\tG_B, 1.20441\n",
      "\tcycle_A, 1.47841\n",
      "\tcycle_B, 3.22890\n",
      "\tidt_A, 1.61095\n",
      "\tidt_B, 0.73826\n",
      "learning rate 3.56e-05 -> 3.37e-05\n",
      "\n",
      "184 / 200\n",
      "\tD_A, 0.13930\n",
      "\tD_B, 0.50670\n",
      "\tG_A, 2.33723\n",
      "\tG_B, 0.65329\n",
      "\tcycle_A, 2.82037\n",
      "\tcycle_B, 3.22389\n",
      "\tidt_A, 1.60998\n",
      "\tidt_B, 1.45722\n",
      "learning rate 3.37e-05 -> 3.17e-05\n",
      "\n",
      "185 / 200\n",
      "\tD_A, 0.14879\n",
      "\tD_B, 0.47339\n",
      "\tG_A, 9.50207\n",
      "\tG_B, 0.63461\n",
      "\tcycle_A, 1.46309\n",
      "\tcycle_B, 3.23048\n",
      "\tidt_A, 1.61327\n",
      "\tidt_B, 0.71667\n",
      "learning rate 3.17e-05 -> 2.97e-05\n",
      "\n",
      "186 / 200\n",
      "\tD_A, 0.12561\n",
      "\tD_B, 0.49663\n",
      "\tG_A, 8.00821\n",
      "\tG_B, 0.69243\n",
      "\tcycle_A, 1.90627\n",
      "\tcycle_B, 3.23400\n",
      "\tidt_A, 1.61401\n",
      "\tidt_B, 0.95384\n",
      "learning rate 2.97e-05 -> 2.77e-05\n",
      "\n",
      "187 / 200\n",
      "\tD_A, 0.08846\n",
      "\tD_B, 0.55007\n",
      "\tG_A, 3.68981\n",
      "\tG_B, 0.52742\n",
      "\tcycle_A, 2.18612\n",
      "\tcycle_B, 3.22593\n",
      "\tidt_A, 1.61103\n",
      "\tidt_B, 1.12047\n",
      "learning rate 2.77e-05 -> 2.57e-05\n",
      "\n",
      "188 / 200\n",
      "\tD_A, 0.09260\n",
      "\tD_B, 0.65845\n",
      "\tG_A, 4.67456\n",
      "\tG_B, 1.05632\n",
      "\tcycle_A, 1.94148\n",
      "\tcycle_B, 3.23319\n",
      "\tidt_A, 1.61542\n",
      "\tidt_B, 0.96199\n",
      "learning rate 2.57e-05 -> 2.38e-05\n",
      "\n",
      "189 / 200\n",
      "\tD_A, 0.08917\n",
      "\tD_B, 0.36851\n",
      "\tG_A, 3.11529\n",
      "\tG_B, 1.02923\n",
      "\tcycle_A, 3.04381\n",
      "\tcycle_B, 3.23798\n",
      "\tidt_A, 1.61738\n",
      "\tidt_B, 1.58484\n",
      "learning rate 2.38e-05 -> 2.18e-05\n",
      "\n",
      "190 / 200\n",
      "\tD_A, 0.07181\n",
      "\tD_B, 0.55835\n",
      "\tG_A, 4.56342\n",
      "\tG_B, 0.42724\n",
      "\tcycle_A, 1.74187\n",
      "\tcycle_B, 3.24322\n",
      "\tidt_A, 1.61950\n",
      "\tidt_B, 0.87008\n",
      "learning rate 2.18e-05 -> 1.98e-05\n",
      "\n",
      "191 / 200\n",
      "\tD_A, 0.08342\n",
      "\tD_B, 0.93610\n",
      "\tG_A, 8.11395\n",
      "\tG_B, 0.81031\n",
      "\tcycle_A, 1.72041\n",
      "\tcycle_B, 3.24515\n",
      "\tidt_A, 1.62131\n",
      "\tidt_B, 0.85964\n",
      "learning rate 1.98e-05 -> 1.78e-05\n",
      "\n",
      "192 / 200\n",
      "\tD_A, 0.10087\n",
      "\tD_B, 0.67364\n",
      "\tG_A, 5.18237\n",
      "\tG_B, 0.99704\n",
      "\tcycle_A, 1.52938\n",
      "\tcycle_B, 3.24537\n",
      "\tidt_A, 1.62053\n",
      "\tidt_B, 0.77306\n",
      "learning rate 1.78e-05 -> 1.58e-05\n",
      "\n",
      "193 / 200\n",
      "\tD_A, 0.06387\n",
      "\tD_B, 0.49784\n",
      "\tG_A, 3.39239\n",
      "\tG_B, 0.70191\n",
      "\tcycle_A, 2.42503\n",
      "\tcycle_B, 3.24314\n",
      "\tidt_A, 1.61903\n",
      "\tidt_B, 1.23968\n",
      "learning rate 1.58e-05 -> 1.39e-05\n",
      "\n",
      "194 / 200\n",
      "\tD_A, 0.12308\n",
      "\tD_B, 1.22188\n",
      "\tG_A, 8.19876\n",
      "\tG_B, 1.08975\n",
      "\tcycle_A, 2.33066\n",
      "\tcycle_B, 3.24678\n",
      "\tidt_A, 1.62123\n",
      "\tidt_B, 1.14826\n",
      "learning rate 1.39e-05 -> 1.19e-05\n",
      "\n",
      "195 / 200\n",
      "\tD_A, 0.05894\n",
      "\tD_B, 1.23263\n",
      "\tG_A, 7.84378\n",
      "\tG_B, 0.89824\n",
      "\tcycle_A, 2.97276\n",
      "\tcycle_B, 3.24717\n",
      "\tidt_A, 1.62141\n",
      "\tidt_B, 1.58415\n",
      "learning rate 1.19e-05 -> 9.90e-06\n",
      "\n",
      "196 / 200\n",
      "\tD_A, 0.08087\n",
      "\tD_B, 0.57769\n",
      "\tG_A, 6.14661\n",
      "\tG_B, 0.64484\n",
      "\tcycle_A, 1.58625\n",
      "\tcycle_B, 3.25207\n",
      "\tidt_A, 1.62420\n",
      "\tidt_B, 0.79283\n",
      "learning rate 9.90e-06 -> 7.92e-06\n",
      "\n",
      "197 / 200\n",
      "\tD_A, 0.07302\n",
      "\tD_B, 0.81425\n",
      "\tG_A, 5.58157\n",
      "\tG_B, 0.73445\n",
      "\tcycle_A, 2.48192\n",
      "\tcycle_B, 3.25184\n",
      "\tidt_A, 1.62353\n",
      "\tidt_B, 1.27787\n",
      "learning rate 7.92e-06 -> 5.94e-06\n",
      "\n",
      "198 / 200\n",
      "\tD_A, 0.05275\n",
      "\tD_B, 0.71174\n",
      "\tG_A, 5.28375\n",
      "\tG_B, 0.89570\n",
      "\tcycle_A, 2.40714\n",
      "\tcycle_B, 3.25005\n",
      "\tidt_A, 1.62296\n",
      "\tidt_B, 1.24290\n",
      "learning rate 5.94e-06 -> 3.96e-06\n",
      "\n",
      "199 / 200\n",
      "\tD_A, 0.05172\n",
      "\tD_B, 0.55454\n",
      "\tG_A, 5.57906\n",
      "\tG_B, 0.86740\n",
      "\tcycle_A, 2.13546\n",
      "\tcycle_B, 3.25074\n",
      "\tidt_A, 1.62344\n",
      "\tidt_B, 1.09557\n",
      "learning rate 3.96e-06 -> 1.98e-06\n",
      "\n",
      "200 / 200\n",
      "\tD_A, 0.07278\n",
      "\tD_B, 0.47788\n",
      "\tG_A, 4.14903\n",
      "\tG_B, 0.80745\n",
      "\tcycle_A, 2.19595\n",
      "\tcycle_B, 3.25016\n",
      "\tidt_A, 1.62311\n",
      "\tidt_B, 1.13251\n",
      "learning rate 1.98e-06 -> 0.00e+00\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    print(f'\\n{epoch + 1} / {epochs}')\n",
    "    \n",
    "    for i, data in enumerate(dataloader):\n",
    "        model.set_input(data)\n",
    "        model.optimize_parameters() # run forward inside\n",
    "        losses = model.get_current_losses()\n",
    "        if i == len(dataloader) - 1:\n",
    "            for key, val in losses.items():\n",
    "                print(f'\\t{key}, {val:.5f}')\n",
    "                \n",
    "    model.update_learning_rate()\n",
    "    if epoch % 20 == 0:\n",
    "        save_suffix = f'epoch_{epoch}'\n",
    "        model.save_networks(save_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2298989295959473\n",
      "3.2298989295959473\n"
     ]
    }
   ],
   "source": [
    "memory_all = torch.cuda.max_memory_allocated(device=None)\n",
    "memory_cuda = torch.cuda.max_memory_allocated(device='cuda')\n",
    "print(memory_all/1024 ** 3)\n",
    "print(memory_cuda/1024 ** 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yi_test",
   "language": "python",
   "name": "yi_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
