{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cycle GAN\n",
    "A simpler CycleGAN constructed following [this repo](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix).\n",
    "\n",
    "Modify it to handle the single-channeled TPC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from torch.nn import init\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "# from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sys.path.append('/sdcc/u/yhuang2/PROJs/GAN/collisionGAN/utils')\n",
    "import network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/sdcc/u/yhuang2/PROJs/GAN/collisionGAN//models')\n",
    "from resnet import ResnetGenerator\n",
    "from alexnet import AlexNetDiscriminator\n",
    "from base_model import BaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple option class\n",
    "(Let us get fancier later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class options:\n",
    "    \"\"\"\n",
    "    I just list all the options here \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # data\n",
    "        self.dataroot = '/sdcc/u/yhuang2/PROJs/GAN/datasets/ls4gan/toyzero_cropped/toyzero_2021-06-29_safi_U/'\n",
    "        self.phase = 'train'\n",
    "        assert self.phase in ['train', 'test'], \"Invalid phase, choose from ['train', 'test']\"\n",
    "        self.max_dataset_size = 1000\n",
    "        \n",
    "        # base_model\n",
    "        self.cuda=True\n",
    "        self.isTrain = True if self.phase == 'train' else False\n",
    "        self.checkpoint_dir = '/sdcc/u/yhuang2/PROJs/GAN/collisionGAN/checkpoints'\n",
    "        self.experiment_name = 'experiment'\n",
    "        \n",
    "        # Training\n",
    "        self.netG = 'resnet_6blocks'\n",
    "        self.batch_size = 4\n",
    "        self.num_workers = 1\n",
    "        self.input_nc = 1\n",
    "        self.output_nc = 1\n",
    "        self.ngf = 64\n",
    "        self.norm_type = 'instance'\n",
    "        self.use_dropout = False\n",
    "        self.gan_mode = 'lsgan'\n",
    "        self.lr = 0.0002\n",
    "        self.lr_policy = 'linear'\n",
    "        self.lr_linear = 100\n",
    "        self.lr_step = 50\n",
    "        self.epochs = 200\n",
    "        self.beta1 = .5\n",
    "        \n",
    "        self.lambda_A = 10\n",
    "        self.lambda_B = 10\n",
    "        self.lambda_identity = .5\n",
    "        self.pool_size = 50\n",
    "        \n",
    "opt = options()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct generators and discriminators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_G(\n",
    "    input_nc,\n",
    "    output_nc,\n",
    "    ngf,\n",
    "    netG,\n",
    "    norm_type='instance', \n",
    "    use_dropout=False, \n",
    "    cuda=True,\n",
    "    net_name='network',\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create a generator:\n",
    "    \n",
    "    Parameters:\n",
    "        1. input_nc (int): number of input channels\n",
    "        2. output_nc (int): number of output channels\n",
    "        3. ngf (int): base number of filters in the conv layers\n",
    "        4. netG (str), the architecture's name: resnet_9blocks | resnet_6blocks \n",
    "            (I will implement remaining later),\n",
    "        5. norm_type (str): the name of the normalization: instance | batch | none\n",
    "        6. use_dropout (bool): whether to use dropout.\n",
    "    \n",
    "    Returns: A generator\n",
    "    \"\"\"\n",
    "    \n",
    "    # net = None,\n",
    "    norm_layer = network.get_norm_layer(norm_type=norm_type)\n",
    "    \n",
    "    if netG == 'resent_9blocks':\n",
    "        net = ResnetGenerator(\n",
    "            input_nc, \n",
    "            output_nc, \n",
    "            ngf, \n",
    "            norm_layer=norm_layer, \n",
    "            use_dropout=use_dropout,\n",
    "            n_blocks=9\n",
    "        )\n",
    "    elif netG == 'resnet_6blocks':\n",
    "        net = ResnetGenerator(\n",
    "            input_nc, \n",
    "            output_nc, \n",
    "            ngf, \n",
    "            norm_layer=norm_layer, \n",
    "            use_dropout=use_dropout,\n",
    "            n_blocks=6\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(f'Generator model name {netG} is not implemented')\n",
    "    \n",
    "    if cuda:\n",
    "        assert torch.cuda.is_available(), \"Cuda is not available\"\n",
    "        net.to('cuda')\n",
    "    network.init_weights(net, net_name=net_name)\n",
    "    \n",
    "    return net\n",
    "\n",
    "\n",
    "def define_D(input_nc, cuda=True, net_name='network'):\n",
    "    \"\"\"\n",
    "    Create an very simple AlexNet discriminator\n",
    "\n",
    "    Parameters:\n",
    "        1. input_nc (int): number of channels in input images\n",
    "        \n",
    "    Returns: A discriminator\n",
    "    \"\"\"\n",
    "    net = AlexNetDiscriminator(input_channels=input_nc)\n",
    "    if cuda:\n",
    "        assert torch.cuda.is_available(), \"Cuda is not available\"\n",
    "        net.to('cuda')\n",
    "    network.init_weights(net, net_name=net_name)\n",
    "    return net\n",
    "\n",
    "\n",
    "# def define_D(\n",
    "#     input_nc, \n",
    "#     ndf,\n",
    "#     netD,\n",
    "#     n_layer=3,\n",
    "#     norm_type='instance',\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Create a discriminator\n",
    "\n",
    "#     Parameters:\n",
    "#         1. input_nc (int): number of channels in input images\n",
    "#         2. ndf (int): number of filters in the first conv layer\n",
    "#         3. netD (str): the architecture's name: basic | n_layers | pixel\n",
    "#         4.n_layers_D (int): the number of conv layers in the discriminator; \n",
    "#             effective when netD=='n_layers'\n",
    "#         5. norm_type (str): the type of normalization layers used in the network.\n",
    "\n",
    "#     Returns: A discriminator\n",
    "#     \"\"\"\n",
    "    \n",
    "#     norm_layer = network.get_norm_layer(norm_type=norm_type)\n",
    "#     net = NLayerDiscriminator(input_nc, ndf, n_layers=n_layers, norm_layer=norm_layer)\n",
    "#     return network.init_weight(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Define different GAN objectives.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gan_mode):\n",
    "        \"\"\" \n",
    "        Initialize the GANLoss class.\n",
    "\n",
    "        Parameters:\n",
    "            gan_mode (str) -- the type of GAN objective. \n",
    "                It currently supports \n",
    "                    - vanilla; \n",
    "                    - lsgan; \n",
    "                    - wgangp.\n",
    "\n",
    "        NOTE: DO NOT use sigmoid as the last layer of Discriminator.\n",
    "            - vanilla handles it with BCEWithLogitsLoss.\n",
    "            - lsgan needs no sigmoid. \n",
    "        \"\"\"\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.register_buffer('real_label', torch.tensor(1.0))\n",
    "        self.register_buffer('fake_label', torch.tensor(0.0))\n",
    "        self.gan_mode = gan_mode\n",
    "        if gan_mode == 'lsgan':\n",
    "            self.loss = nn.MSELoss()\n",
    "        elif gan_mode == 'vanilla':\n",
    "            self.loss = nn.BCEWithLogitsLoss()\n",
    "        elif gan_mode == 'wgangp':\n",
    "            self.loss = None\n",
    "        else:\n",
    "            raise NotImplementedError('gan mode %s not implemented' % gan_mode)\n",
    "\n",
    "    def __call__(self, prediction, target_is_real):\n",
    "        \"\"\"\n",
    "        Calculate loss given Discriminator's output and grount truth labels.\n",
    "\n",
    "        Parameters:\n",
    "            prediction (tensor) -- prediction from a discriminator\n",
    "            target_is_real (bool) -- if the ground truth label is for real images or fake images\n",
    "\n",
    "        Returns:\n",
    "            the calculated loss.\n",
    "        \"\"\"\n",
    "        if self.gan_mode in ['lsgan', 'vanilla']:\n",
    "            target_tensor = self.real_label if target_is_real else self.fake_label\n",
    "            loss = self.loss(prediction, target_tensor.expand_as(prediction))\n",
    "        elif self.gan_mode == 'wgangp':\n",
    "            L = prediction.mean()\n",
    "            loss = -L if target_is_real else L\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGAN(BaseModel):\n",
    "    def __init__(self, opt):\n",
    "        super(CycleGAN, self).__init__(opt)\n",
    "        self.device = 'cuda' if opt.cuda else 'cpu'\n",
    "        \n",
    "        self.loss_names = ['D_A', 'G_A', 'cycle_A', 'idt_A', 'D_B', 'G_B', 'cycle_B', 'idt_B']\n",
    "        if self.isTrain:\n",
    "            self.model_names = ['G_A', 'G_B', 'D_A', 'D_B']\n",
    "        else:\n",
    "            self.model_names = ['G_A', 'G_B']\n",
    "            \n",
    "        # generator networks\n",
    "        self.netG_A = define_G(\n",
    "            opt.input_nc,\n",
    "            opt.output_nc,\n",
    "            opt.ngf,\n",
    "            opt.netG,\n",
    "            opt.norm_type,\n",
    "            opt.use_dropout,\n",
    "            cuda=opt.cuda,\n",
    "            net_name='generator A->B',\n",
    "        )\n",
    "        self.netG_B = define_G(\n",
    "            opt.output_nc,\n",
    "            opt.input_nc, \n",
    "            opt.ngf,\n",
    "            opt.netG,\n",
    "            opt.norm_type,\n",
    "            opt.use_dropout,\n",
    "            cuda=opt.cuda,\n",
    "            net_name='generator B->A',\n",
    "        )\n",
    "        \n",
    "        # discriminator networks, criterions, and optimizers for training\n",
    "        if opt.isTrain:\n",
    "#             self.netD_A = define_D(\n",
    "#                 opt.output_nc,\n",
    "#                 opt.ndf, # number of discriminator filters in the first conv layer\n",
    "#                 opt.netD,\n",
    "#                 opt.n_layers_D,\n",
    "#                 opt.norm_type,\n",
    "#             )\n",
    "#             self.netD_B = define_D(\n",
    "#                 opt.input_nc,\n",
    "#                 opt.ndf, # number of discriminator filters in the first conv layer\n",
    "#                 opt.netD,\n",
    "#                 opt.n_layers_D,\n",
    "#                 opt.norm_type,\n",
    "#             )\n",
    "            self.netD_A = define_D(opt.output_nc, cuda=opt.cuda, net_name='discriminator A')\n",
    "            self.netD_B = define_D(opt.input_nc, cuda=opt.cuda, net_name='discriminator B')\n",
    "            \n",
    "            if opt.lambda_identity > 0:\n",
    "                assert(opt.input_nc == opt.output_nc)\n",
    "            \n",
    "            # fake images buffers\n",
    "            self.fake_A_pool = network.ImagePool(opt.pool_size)\n",
    "            self.fake_B_pool = network.ImagePool(opt.pool_size)\n",
    "            \n",
    "            # Loss functions\n",
    "            self.criterionGAN = GANLoss(opt.gan_mode).to(self.device)\n",
    "            self.criterionCycle = torch.nn.L1Loss()\n",
    "            self.criterionIdt = torch.nn.L1Loss()\n",
    "            \n",
    "            # Optimizers\n",
    "            self.optimizer_G = torch.optim.Adam(\n",
    "                itertools.chain(self.netG_A.parameters(), self.netG_B.parameters()), \n",
    "                lr=opt.lr, \n",
    "                betas=(opt.beta1, .999)\n",
    "            )\n",
    "            self.optimizer_D = torch.optim.Adam(\n",
    "                itertools.chain(self.netD_A.parameters(), self.netD_B.parameters()), \n",
    "                lr=opt.lr, \n",
    "                betas=(opt.beta1, .999)\n",
    "            )\n",
    "            self.optimizers = [self.optimizer_G, self.optimizer_D]\n",
    "            if self.isTrain:\n",
    "                self.schedulers = [network.get_scheduler(optimizer, opt) for optimizer in self.optimizers]\n",
    "\n",
    "    def set_input(self, data):\n",
    "        self.real_A = data['A'].to(self.device)\n",
    "        self.real_B = data['B'].to(self.device)\n",
    "   \n",
    "    def forward(self):\n",
    "        self.fake_B = self.netG_A(self.real_A) # G_A(A)\n",
    "        self.rec_A = self.netG_B(self.fake_B)  # G_B(G_A(A))\n",
    "        self.fake_A = self.netG_B(self.real_B) # G_B(B)\n",
    "        self.rec_B = self.netG_A(self.fake_A)  # G_A(G_B(B))\n",
    "        \n",
    "    def backward_D_basic(self, netD, real, fake):\n",
    "        \"\"\"\n",
    "        Calculate GAN loss for the discriminator\n",
    "        \n",
    "        Parameters:\n",
    "            1. netD (network): the discriminator D\n",
    "            2. real (tensor): real images\n",
    "            3. fake (tensor): images generated by a generator\n",
    "        \n",
    "        Retrun:\n",
    "            the discriminator loss.\n",
    "        \"\"\"\n",
    "        # Real\n",
    "        pred_real = netD(real)\n",
    "        loss_D_real = self.criterionGAN(pred_real, True)\n",
    "        \n",
    "        # Fake\n",
    "        pred_fake = netD(fake.detach())\n",
    "        loss_D_fake = self.criterionGAN(pred_fake, False)\n",
    "#         print(loss_D_real)\n",
    "#         print(loss_D_fake)\n",
    "        \n",
    "        # Combine loss and calculate gradients\n",
    "        loss_D = (loss_D_real + loss_D_fake) * .5\n",
    "        loss_D.backward()\n",
    "        \n",
    "        return loss_D\n",
    "\n",
    "    def backward_D_A(self):\n",
    "        \"\"\"\n",
    "        Calculate GAN loss for the discriminator D_A\n",
    "        \"\"\"\n",
    "        self.loss_D_A = self.backward_D_basic(\n",
    "            self.netD_A,\n",
    "            self.real_B,\n",
    "            # the collection of fake images is a \n",
    "            # mixture of old and new fake images\n",
    "            self.fake_B_pool.query(self.fake_B)\n",
    "        )\n",
    "\n",
    "    def backward_D_B(self):\n",
    "        \"\"\"\n",
    "        Calculate GAN loss for the discriminator D_B\n",
    "        \"\"\"\n",
    "        self.loss_D_B = self.backward_D_basic(\n",
    "            self.netD_B,\n",
    "            self.real_A,\n",
    "            # the collection of fake images is a \n",
    "            # mixture of old and new fake images\n",
    "            self.fake_A_pool.query(self.fake_A)\n",
    "        )\n",
    "\n",
    "    def backward_G(self):\n",
    "        \"\"\"\n",
    "        Calculate the loss for generators G_A and G_B\n",
    "        \"\"\"\n",
    "        lambda_idt = self.opt.lambda_identity # it is a multplier to lambda_A and lambda_B\n",
    "        lambda_A, lambda_B = self.opt.lambda_A, self.opt.lambda_B\n",
    "        \n",
    "        # Identity loss\n",
    "        self.loss_idt_A, self.loss_idt_B = 0, 0\n",
    "        if lambda_idt > 0:\n",
    "            self.idt_A = self.netG_A(self.real_B)\n",
    "            self.loss_idt_A = self.criterionIdt(self.idt_A, self.real_B) * lambda_B * lambda_idt\n",
    "            self.idt_B = self.netG_B(self.real_A)\n",
    "            self.loss_idt_B = self.criterionIdt(self.idt_B, self.real_A) * lambda_A * lambda_idt\n",
    "        \n",
    "        # GAN losses\n",
    "        self.loss_G_A = self.criterionGAN(self.netD_A(self.fake_B), True)\n",
    "        self.loss_G_B = self.criterionGAN(self.netD_B(self.fake_A), True)\n",
    "        \n",
    "        # Cycle losses:\n",
    "        self.loss_cycle_A = self.criterionCycle(self.rec_A, self.real_A) * lambda_A\n",
    "        self.loss_cycle_B = self.criterionCycle(self.rec_B, self.real_B) * lambda_B\n",
    "        \n",
    "        # Combine and backpropagate\n",
    "        self.loss_G = self.loss_G_A + self.loss_G_B \\\n",
    "                    + self.loss_cycle_A + self.loss_cycle_B \\\n",
    "                    + self.loss_idt_A + self.loss_idt_B\n",
    "        self.loss_G.backward()\n",
    " \n",
    "    def optimize_parameters(self):\n",
    "        \"\"\"\n",
    "        Calculate losses and gradients, and update network weights.\n",
    "        \"\"\"\n",
    "        self.forward()\n",
    "        \n",
    "        # Optimize generators\n",
    "        for net in [self.netD_A, self.netD_B]:\n",
    "            for param in net.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.optimizer_G.zero_grad()\n",
    "        self.backward_G()\n",
    "        self.optimizer_G.step()\n",
    "        \n",
    "        # Optimize discriminators\n",
    "        for net in [self.netD_A, self.netD_B]:\n",
    "            for param in net.parameters():\n",
    "                param.requires_grad = True\n",
    "                \n",
    "        self.optimizer_D.zero_grad()\n",
    "        self.backward_D_A()\n",
    "        self.backward_D_B()\n",
    "        self.optimizer_D.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_fnames(dirname, max_dataset_size=float('inf')):\n",
    "    \"\"\"\n",
    "    load image fnames.\n",
    "    If max_dataset_size is not infinity and is less than all available images,\n",
    "    return a random subset of max_dataset_size image fnames.\n",
    "    \"\"\"\n",
    "    assert Path(dirname).exists(), f\"{dirname} doesn't exist\"\n",
    "    image_fnames = np.array(list(Path(dirname).glob('*npz')))\n",
    "    \n",
    "    if max_dataset_size != float('inf') and max_dataset_size < len(image_fnames):\n",
    "        indices = np.arange(len(image_fnames))\n",
    "        np.random.shuffle(indices)\n",
    "        indices = indices[:max_dataset_size]\n",
    "        image_fnames = image_fnames[indices]\n",
    "    return image_fnames\n",
    "\n",
    "\n",
    "class toyzero_dataset(Dataset):\n",
    "    def __init__(self, opt):\n",
    "        super(toyzero_dataset, self).__init__()\n",
    "        dir_A = Path(opt.dataroot)/f'{opt.phase}A'\n",
    "        dir_B = Path(opt.dataroot)/f'{opt.phase}B'\n",
    "        self.image_fnames_A = load_image_fnames(dir_A, opt.max_dataset_size)\n",
    "        self.image_fnames_B = load_image_fnames(dir_B, opt.max_dataset_size)\n",
    "        self.size_A = len(self.image_fnames_A)\n",
    "        self.size_B = len(self.image_fnames_B)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(self.size_A, self.size_B)\n",
    "    \n",
    "    def _load(self, image_fname):\n",
    "        image = np.load(image_fname)\n",
    "        image = image[image.files[0]]\n",
    "        image = np.expand_dims(np.float32(image), 0)\n",
    "        return image\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        index_A = index % self.size_A\n",
    "        index_B = np.random.randint(0, self.size_B - 1) # inclusive end\n",
    "        image_A = self._load(self.image_fnames_A[index_A])\n",
    "        image_B = self._load(self.image_fnames_B[index_B])\n",
    "        \n",
    "        return {'A': image_A, 'B': image_B}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = toyzero_dataset(opt)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=opt.batch_size,\n",
    "    num_workers=opt.num_workers,\n",
    "    shuffle=True\n",
    ")\n",
    "# for data in dataloader[:4]:\n",
    "#     print(data['A'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize generator A->B with normal distribution\n",
      "initialize generator B->A with normal distribution\n",
      "initialize discriminator A with normal distribution\n",
      "initialize discriminator B with normal distribution\n"
     ]
    }
   ],
   "source": [
    "model = CycleGAN(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 2.00e-04 -> 2.00e-04\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    model.update_learning_rate()\n",
    "    for i, data in enumerate(dataloader):\n",
    "        model.set_input(data)\n",
    "        model.optimize_parameters() # run forward inside\n",
    "        losses = model.get_current_losses()\n",
    "    if epoch % 20 == 19:\n",
    "        save_suffix = f'epoch_{epoch}'\n",
    "        model.save_network(save_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yi_test",
   "language": "python",
   "name": "yi_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
